{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from nltk.corpus import wordnet\n",
    "import spacy\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('csv/processed_tickets_en.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>processed_text</th>\n",
       "      <th>type</th>\n",
       "      <th>queue</th>\n",
       "      <th>priority</th>\n",
       "      <th>urgency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>request server administration assistance dear ...</td>\n",
       "      <td>Request</td>\n",
       "      <td>Product Support</td>\n",
       "      <td>medium</td>\n",
       "      <td>not_urgent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>inquiry macbook air m1 performance dear tech o...</td>\n",
       "      <td>Request</td>\n",
       "      <td>Customer Service</td>\n",
       "      <td>low</td>\n",
       "      <td>not_urgent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>immediate assistance require touchscreen respo...</td>\n",
       "      <td>Incident</td>\n",
       "      <td>Product Support</td>\n",
       "      <td>high</td>\n",
       "      <td>urgent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>request improvement aw infrastructure setting ...</td>\n",
       "      <td>Change</td>\n",
       "      <td>IT Support</td>\n",
       "      <td>high</td>\n",
       "      <td>urgent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336</th>\n",
       "      <td>request support appreciate help set canon pixm...</td>\n",
       "      <td>Request</td>\n",
       "      <td>Customer Service</td>\n",
       "      <td>low</td>\n",
       "      <td>not_urgent</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        processed_text      type  \\\n",
       "3    request server administration assistance dear ...   Request   \n",
       "202  inquiry macbook air m1 performance dear tech o...   Request   \n",
       "225  immediate assistance require touchscreen respo...  Incident   \n",
       "238  request improvement aw infrastructure setting ...    Change   \n",
       "336  request support appreciate help set canon pixm...   Request   \n",
       "\n",
       "                queue priority     urgency  \n",
       "3     Product Support   medium  not_urgent  \n",
       "202  Customer Service      low  not_urgent  \n",
       "225   Product Support     high      urgent  \n",
       "238        IT Support     high      urgent  \n",
       "336  Customer Service      low  not_urgent  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert high priority to urgent and medium and low to not urgent\n",
    "\n",
    "df['urgency'] = df['priority'].apply(lambda x: 'urgent' if x == 'high' else 'not_urgent')\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(175, 5)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.urgency == 'urgent'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(163, 5)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.urgency == 'not_urgent'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load SpaCy model for lemmatization\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.lemma_.lower() for token in doc if not token.is_stop and not token.is_punct]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# Function for synonym replacement\n",
    "def synonym_replacement(text, n=1):\n",
    "    words = text.split()\n",
    "    new_words = words.copy()\n",
    "    for _ in range(n):\n",
    "        word_to_replace = random.choice(words)\n",
    "        synonyms = wordnet.synsets(word_to_replace)\n",
    "        if synonyms:\n",
    "            synonym = synonyms[0].lemmas()[0].name()\n",
    "            new_words = [synonym if word == word_to_replace else word for word in new_words]\n",
    "    return \" \".join(new_words)\n",
    "\n",
    "# Function for random insertion\n",
    "def random_insertion(text, n=1):\n",
    "    words = text.split()\n",
    "    for _ in range(n):\n",
    "        random_word = random.choice(words)\n",
    "        synonyms = wordnet.synsets(random_word)\n",
    "        if synonyms:\n",
    "            synonym = synonyms[0].lemmas()[0].name()\n",
    "            insert_position = random.randint(0, len(words))\n",
    "            words.insert(insert_position, synonym)\n",
    "    return \" \".join(words)\n",
    "\n",
    "# Function for random deletion\n",
    "def random_deletion(text, p=0.1):\n",
    "    words = text.split()\n",
    "    if len(words) == 1:\n",
    "        return text\n",
    "    new_words = [word for word in words if random.random() > p]\n",
    "    return \" \".join(new_words) if new_words else random.choice(words)\n",
    "\n",
    "# Function for shuffling\n",
    "def shuffle_words(text):\n",
    "    words = text.split()\n",
    "    random.shuffle(words)\n",
    "    return \" \".join(words)\n",
    "\n",
    "# Apply text augmentation\n",
    "augmented_data_map = {}\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    # original data\n",
    "    augmented_data_map[index] = {\n",
    "        'subject': row['subject'],\n",
    "        'body': row['body'],\n",
    "        'priority': row['priority'],\n",
    "        'type': row['type'],\n",
    "        'queue': row['queue']\n",
    "    }\n",
    "    \n",
    "    # synonym replacement\n",
    "    augmented_data_map[f'{index}_synonym'] = {\n",
    "        'subject': synonym_replacement(row['subject']),\n",
    "        'body': synonym_replacement(row['body']),\n",
    "        'priority': row['priority'],\n",
    "        'type': row['type'],\n",
    "        'queue': row['queue']\n",
    "    }\n",
    "    \n",
    "    # random insertion\n",
    "    augmented_data_map[f'{index}_insertion'] = {\n",
    "        'subject': random_insertion(row['subject']),\n",
    "        'body': random_insertion(row['body']),\n",
    "        'priority': row['priority'],\n",
    "        'type': row['type'],\n",
    "        'queue': row['queue']\n",
    "    }\n",
    "    \n",
    "    # random deletion\n",
    "    augmented_data_map[f'{index}_deletion'] = {\n",
    "        'subject': random_deletion(row['subject']),\n",
    "        'body': random_deletion(row['body']),\n",
    "        'priority': row['priority'],\n",
    "        'type': row['type'],\n",
    "        'queue': row['queue']\n",
    "    }\n",
    "    \n",
    "    # shuffling\n",
    "    augmented_data_map[f'{index}_shuffle'] = {\n",
    "        'subject': shuffle_words(row['subject']),\n",
    "        'body': shuffle_words(row['body']),\n",
    "        'priority': row['priority'],\n",
    "        'type': row['type'],\n",
    "        'queue': row['queue']\n",
    "    }\n",
    "    \n",
    "# Create a new DataFrame with augmented data\n",
    "augmented_data_df = pd.DataFrame(augmented_data_map).T\n",
    "    \n",
    "augmented_data_df.sample(3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
