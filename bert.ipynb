{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>processed_text</th>\n",
       "      <th>type</th>\n",
       "      <th>queue</th>\n",
       "      <th>priority</th>\n",
       "      <th>urgency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Urgent: Critical impact on enterprise network ...</td>\n",
       "      <td>Incident</td>\n",
       "      <td>Technical Support</td>\n",
       "      <td>high</td>\n",
       "      <td>urgent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Intermittent Cursor Freezing Issue on Dell XPS...</td>\n",
       "      <td>Incident</td>\n",
       "      <td>Product Support</td>\n",
       "      <td>low</td>\n",
       "      <td>not_urgent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Urgent: Support needed for data recovery of My...</td>\n",
       "      <td>Incident</td>\n",
       "      <td>Technical Support</td>\n",
       "      <td>high</td>\n",
       "      <td>urgent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Inquiry about MacBook Air M1 features. Dear Cu...</td>\n",
       "      <td>Request</td>\n",
       "      <td>Sales and Pre-Sales</td>\n",
       "      <td>low</td>\n",
       "      <td>not_urgent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Request Assistance with Office 365 Installatio...</td>\n",
       "      <td>Problem</td>\n",
       "      <td>Technical Support</td>\n",
       "      <td>medium</td>\n",
       "      <td>not_urgent</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      processed_text      type  \\\n",
       "0  Urgent: Critical impact on enterprise network ...  Incident   \n",
       "1  Intermittent Cursor Freezing Issue on Dell XPS...  Incident   \n",
       "2  Urgent: Support needed for data recovery of My...  Incident   \n",
       "3  Inquiry about MacBook Air M1 features. Dear Cu...   Request   \n",
       "4  Request Assistance with Office 365 Installatio...   Problem   \n",
       "\n",
       "                 queue priority     urgency  \n",
       "0    Technical Support     high      urgent  \n",
       "1      Product Support      low  not_urgent  \n",
       "2    Technical Support     high      urgent  \n",
       "3  Sales and Pre-Sales      low  not_urgent  \n",
       "4    Technical Support   medium  not_urgent  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('csv/processed_tickets_en.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>processed_text</th>\n",
       "      <th>priority</th>\n",
       "      <th>urgency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>This is a critical issue.</td>\n",
       "      <td>high</td>\n",
       "      <td>urgent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>This is a moderate issue.</td>\n",
       "      <td>medium</td>\n",
       "      <td>not_urgent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>This is a minor issue.</td>\n",
       "      <td>low</td>\n",
       "      <td>not_urgent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>This issue requires immediate attention.</td>\n",
       "      <td>high</td>\n",
       "      <td>urgent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>This issue should be addressed soon.</td>\n",
       "      <td>medium</td>\n",
       "      <td>not_urgent</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                            processed_text priority     urgency\n",
       "0      2                 This is a critical issue.     high      urgent\n",
       "1      1                 This is a moderate issue.   medium  not_urgent\n",
       "2      0                    This is a minor issue.      low  not_urgent\n",
       "3      2  This issue requires immediate attention.     high      urgent\n",
       "4      1      This issue should be addressed soon.   medium  not_urgent"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_hf = pd.read_csv('csv/hf_priority.csv')\n",
    "df_hf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8649, 6)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# concat the two dataframes vertically\n",
    "df_combined = pd.concat([df, df_hf], axis=0)\n",
    "df_combined.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# check the shape of the new dataframe\n",
    "df_combined.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA90AAAJOCAYAAACqS2TfAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8g+/7EAAAACXBIWXMAAA9hAAAPYQGoP6dpAABqYUlEQVR4nO3deVxVdeL/8Teg7IIbi6Qiaim4i6VMpqQkGTlaTouZu5WGlkvqOJm7Q1lulcuUJTZpmjOppabihqmYRuKCRmr6xVLQSiA3UDi/P/pxxitIghwv5uv5eJyHnvP5nM/5nMu9fO6bszkYhmEIAAAAAACUOkd7dwAAAAAAgD8rQjcAAAAAABYhdAMAAAAAYBFCNwAAAAAAFiF0AwAAAABgEUI3AAAAAAAWIXQDAAAAAGARQjcAAAAAABYhdAMAAAAAYBFCN3AL9O7dW7Vq1Sr1dh0cHDR+/PhSb7c01KpVS71797Z8O8ePH5eDg4NiY2PNZb1795anp6fl285Xln8OAAAUV2Fjq1ViY2Pl4OCg48ePm8tq1aqlRx991PJtS9KWLVvk4OCgLVu23JLt4c5E6Ab+QP5gkD+5urrqnnvu0aBBg5Senm7v7tnYsWOHxo8fr4yMjFJtNzw83Nx/R0dHeXl5qV69eurRo4fi4uJKbTtr1qwps+G1LPcNAO5E48ePl4ODg37++edCyxs2bKjw8PBb26ky6urvMeXKlVPlypUVGhqql19+WQcPHiy17cyZM+eWBPWSKMt9w59fOXt3ALhdTJw4UUFBQbp06ZK2bdumuXPnas2aNTpw4IDc3d2LXPf9999XXl5eqffp4sWLKlfufx/jHTt2aMKECerdu7cqVqxYqtuqXr26YmJiJEnnz5/XkSNH9Nlnn+njjz/Wk08+qY8//ljly5c366ekpMjRsXh/11uzZo1mz55drHAbGBioixcv2mzbCkX17dqfAwAAZc1DDz2knj17yjAMZWZmau/evVq4cKHmzJmjN954Q8OGDTPrlnRsnTNnjqpWrVqsM9169Oihp59+Wi4uLsXaVnFdr29t2rTRxYsX5ezsbOn2cWfjWyJwgzp27KgWLVpIkvr3768qVapo+vTpWrlypbp161boOufPn5eHh0epBsK8vDzl5OTI1dVVrq6updbuH/H29tazzz5rs+z111/XSy+9pDlz5qhWrVp64403zDKrB88rV64oLy9Pzs7Ot/R1KIy9tw8AKJlLly7J2dm52H8kvh3dc889hY7jnTp10vDhw1W/fn098sgjkmSe2Wel/O9ITk5OcnJysnRbRXF0dGQch+X+/L9hAIu0a9dOknTs2DFJ/7uO+OjRo3rkkUdUoUIFde/e3Sy79pru8+fPa/jw4apRo4ZcXFxUr149vfXWWzIMw6aeg4ODBg0apEWLFqlBgwZycXHR2rVrzbL8I6/jx4/XiBEjJElBQUHmaWTHjx9X27Zt1aRJk0L3o169eoqMjCzRa+Dk5KS3335bISEhevfdd5WZmWmWXXtN9+XLlzVhwgTdfffdcnV1VZUqVdS6dWvz9PTevXtr9uzZ5n7lT9L/ri176623NHPmTNWpU0cuLi46ePBgkded/fDDD4qMjJSHh4cCAgI0ceJEm9f3etdxXdtmUX3LX3btEfA9e/aoY8eO8vLykqenp9q3b6+dO3fa1Mm/dGH79u0aNmyYfHx85OHhoccee0xnzpz54x8AAOCG5f/OX7JkicaMGaO77rpL7u7uysrKkiQtW7ZMISEhcnV1VcOGDbV8+fJCx++8vDzNnDlTDRo0kKurq/z8/PTCCy/o7NmzNvXyr0vetm2b7rvvPrm6uqp27dr66KOPCvQtIyNDQ4cOVa1ateTi4qLq1aurZ8+e+vnnn3Xu3Dl5eHjo5ZdfLrDejz/+KCcnJ/NMtOKqUqWKlixZonLlymnKlCnm8sLG1rS0NPXp00fVq1eXi4uLqlWrps6dO5vXYteqVUvJycmKj483x8n80/vzx7v4+Hi9+OKL8vX1VfXq1W3Krr6mO9/69evVtGlTubq6KiQkRJ999plNef4lBte6ts2i+na97wLLli1TaGio3NzcVLVqVT377LP66aefbOrkf/f76aef1KVLF3l6esrHx0evvPKKcnNz/+DVx52EI91ACR09elTS7wNWvitXrigyMlKtW7fWW2+9dd3Tzg3D0F//+ldt3rxZ/fr1U9OmTbVu3TqNGDFCP/30k2bMmGFTf9OmTfr00081aNAgVa1atdCbsj3++OP6/vvv9cknn2jGjBmqWrWqJMnHx0c9evTQc889pwMHDqhhw4bmOrt379b333+vMWPGlPh1cHJyUrdu3fTaa69p27ZtioqKKrTe+PHjFRMTo/79++u+++5TVlaWvvnmG3377bd66KGH9MILL+jkyZOKi4vTv//970LbWLBggS5duqTnn39eLi4uqly58nVP28/NzdXDDz+sVq1aaerUqVq7dq3GjRunK1euaOLEicXaxxvp29WSk5P1wAMPyMvLSyNHjlT58uX1r3/9S+Hh4YqPj1fLli1t6g8ePFiVKlXSuHHjdPz4cc2cOVODBg3S0qVLi9VPAMAfmzRpkpydnfXKK68oOztbzs7OWr16tZ566ik1atRIMTExOnv2rPr166e77rqrwPovvPCCYmNj1adPH7300ks6duyY3n33Xe3Zs0fbt2+3ObvtyJEj+tvf/qZ+/fqpV69e+vDDD9W7d2+FhoaqQYMGkqRz587pgQce0KFDh9S3b181b95cP//8sz7//HP9+OOPatq0qR577DEtXbpU06dPtzkq/Mknn8gwDPOP/CVRs2ZNtW3bVps3b1ZWVpa8vLwKrde1a1clJydr8ODBqlWrlk6fPq24uDilpqaqVq1amjlzpgYPHixPT0+9+uqrkiQ/Pz+bNl588UX5+Pho7NixOn/+fJH9Onz4sJ566ikNGDBAvXr10oIFC/TEE09o7dq1euihh4q1jzfSt6vl/3zvvfdexcTEKD09XbNmzdL27du1Z88em0v4cnNzFRkZqZYtW+qtt97Shg0bNG3aNNWpU0cDBw4sVj/xJ2YAKNKCBQsMScaGDRuMM2fOGCdOnDCWLFliVKlSxXBzczN+/PFHwzAMo1evXoYk4+9//3uBNnr16mUEBgaa8ytWrDAkGZMnT7ap97e//c1wcHAwjhw5Yi6TZDg6OhrJyckF2pVkjBs3zpx/8803DUnGsWPHbOplZGQYrq6uxqhRo2yWv/TSS4aHh4dx7ty5Il+Dtm3bGg0aNLhu+fLlyw1JxqxZs8xlgYGBRq9evcz5Jk2aGFFRUUVuJzo62ijs19KxY8cMSYaXl5dx+vTpQssWLFhgLsv/WQwePNhclpeXZ0RFRRnOzs7GmTNnDMMwjM2bNxuSjM2bN/9hm9frm2EU/Dl06dLFcHZ2No4ePWouO3nypFGhQgWjTZs25rL891ZERISRl5dnLh86dKjh5ORkZGRkFLo9AIBhjBs3zpBk/k6/VoMGDYy2bdua8/m/82vXrm1cuHDBpm6jRo2M6tWrG7/99pu5bMuWLYYkm/H7q6++MiQZixYtsll/7dq1BZYHBgYakoytW7eay06fPm24uLgYw4cPN5eNHTvWkGR89tlnBfYhf2xYt26dIcn48ssvbcobN25ss4/XI8mIjo6+bvnLL79sSDL27t1rGEbBcfDs2bOGJOPNN98scjvXvub58se71q1bG1euXCm07OrvLvmv3X//+19zWWZmplGtWjWjWbNm5rL898D1tnd1m9fr27XfBXJycgxfX1+jYcOGxsWLF816q1atMiQZY8eONZflf9+YOHGiTZvNmjUzQkNDC2wLdy5OLwduUEREhHx8fFSjRg09/fTT8vT01PLlywv8FfxG/qq5Zs0aOTk56aWXXrJZPnz4cBmGoS+//NJmedu2bRUSElLivnt7e6tz587mX8Sl3/8yu3TpUnXp0kUeHh4lbluS+Xiu33777bp1KlasqOTkZB0+fLjE2+natat8fHxuuP6gQYPM/+efpp+Tk6MNGzaUuA9/JDc3V+vXr1eXLl1Uu3Ztc3m1atX0zDPPaNu2beapjPmef/55m9PjHnjgAeXm5ur//u//LOsnANypevXqJTc3N3P+5MmT2r9/v3r27GnzuMm2bduqUaNGNusuW7ZM3t7eeuihh/Tzzz+bU2hoqDw9PbV582ab+iEhIXrggQfMeR8fH9WrV08//PCDuey///2vmjRposcee6xAX/PHhoiICAUEBGjRokVm2YEDB7Rv374C12mXxB+N425ubnJ2dtaWLVsKnEZfHM8999wNX78dEBBg85p4eXmpZ8+e2rNnj9LS0krchz/yzTff6PTp03rxxRdtrvWOiopS/fr1tXr16gLrDBgwwGb+gQcesPkZA4Ru4AbNnj1bcXFx2rx5sw4ePGheL3y1cuXKmdcoFeX//u//FBAQoAoVKtgsDw4ONsuvFhQUdJO9l3r27KnU1FR99dVXkqQNGzYoPT1dPXr0uOm2z507J0kF9udqEydOVEZGhu655x41atRII0aM0L59+4q1neK8Do6OjjahV/r9JjKSCr1urLScOXNGFy5cUL169QqUBQcHKy8vTydOnLBZXrNmTZv5SpUqSdJNfbEBAKjQ632vHUvyx9y6desWqHvtssOHDyszM1O+vr7y8fGxmc6dO6fTp0/b1L/297v0++/4q3+/Hz161ObSr8I4Ojqqe/fuWrFihS5cuCBJWrRokVxdXfXEE08Uue6N+KNx3MXFRW+88Ya+/PJL+fn5qU2bNpo6dWqxw29xxvG6desW+PndinE8//1Q2Dhev379At/RXF1dCxwQuPZnDBC6gRt03333KSIiQuHh4QoODi70TqcuLi6W3AH16r/Il1RkZKT8/Pz08ccfS5I+/vhj+fv7KyIi4qbbPnDggKTCv7Dka9OmjY4ePaoPP/xQDRs21Pz589W8eXPNnz//hrdTGq/D1Qr7Mibplt/85Hp/9TeuuakeAOB/8o9CXrx4sdDyCxcuFHpX6psZS/Ly8uTr66u4uLhCp2vvGVKav9979uypc+fOacWKFTIMQ4sXL9ajjz4qb2/vEu3L1Q4cOCAnJ6ciQ/GQIUP0/fffKyYmRq6urnrttdcUHBysPXv23PB2/ozjuD3vvI7bB6EbsIPAwECdPHmywGlc3333nVleEtcbfKTfB4VnnnlG//nPf3T27FmtWLFC3bp1u+nBIjc3V4sXL5a7u7tat25dZN3KlSurT58++uSTT3TixAk1btzY5q7fRfW/uPLy8gqc2vX9999LknkjuvwjyhkZGTb1Cjut+0b75uPjI3d3d6WkpBQo++677+To6KgaNWrcUFsAgOvLHysL+3174cIFnThx4obG0/w6R44cKVB27bI6derol19+0f3336+IiIgC0/WeFFKUOnXqmH+8LkrDhg3VrFkzLVq0SF999ZVSU1NL5Wy11NRUxcfHKywsrMgz1vL7Onz4cK1fv14HDhxQTk6Opk2bZpaX5jh+5MiRAn+cuBXjeFHvq5SUlBJ/R8OdjdAN2MEjjzyi3NxcvfvuuzbLZ8yYIQcHB3Xs2LFE7eZfm33t4JOvR48eOnv2rF544QWdO3fupq8Dy83N1UsvvaRDhw7ppZdeuu4dTyXpl19+sZn39PRU3bp1lZ2dfcP9L66rX1/DMPTuu++qfPnyat++vaTfB1YnJydt3brVZr05c+YUaOtG++bk5KQOHTpo5cqVNqe/paena/HixWrdunWRrxMA4Ma0b99ezs7Omjt3boEnWbz33nu6cuXKDY2nAQEBatiwoT766CPzNGtJio+P1/79+23qPvnkk8rNzdWkSZMKtHPlypUSjV9du3bV3r17tXz58gJl14bOHj16aP369Zo5c6aqVKlS4u8L+X799Vd169ZNubm55l29C3PhwgVdunTJZlmdOnVUoUKFAuN4aY3hJ0+etHlNsrKy9NFHH6lp06by9/c3+yDJZhw/f/68Fi5cWKC9G+1bixYt5Ovrq3nz5tns25dffqlDhw5d9yktQFF4ZBhgB506ddKDDz6oV199VcePH1eTJk20fv16rVy5UkOGDDEHkeIKDQ2VJL366qt6+umnVb58eXXq1MkMjM2aNVPDhg21bNkyBQcHq3nz5jfcdmZmpnlq+oULF3TkyBF99tlnOnr0qJ5++ulCv4BcLSQkROHh4QoNDVXlypX1zTff6D//+Y/Nzc7y+//SSy8pMjJSTk5Oevrpp4v1GuRzdXXV2rVr1atXL7Vs2VJffvmlVq9erX/84x/mtVfe3t564okn9M4778jBwUF16tTRqlWrClyTV9y+TZ48WXFxcWrdurVefPFFlStXTv/617+UnZ2tqVOnlmh/AAC2fH19NXbsWI0ZM0Zt2rTRX//6V7m7u2vHjh365JNP1KFDB3Xq1OmG2vrnP/+pzp076/7771efPn109uxZvfvuu2rYsKFNEG/btq1eeOEFxcTEKCkpSR06dFD58uV1+PBhLVu2TLNmzdLf/va3Yu3HiBEj9J///EdPPPGE+vbtq9DQUP3666/6/PPPNW/ePJuj588884xGjhyp5cuXa+DAgTaPJ/sj33//vT7++GMZhqGsrCzt3btXy5Yt07lz5zR9+nQ9/PDDRa7bvn17PfnkkwoJCVG5cuW0fPlypaen24yFoaGhmjt3riZPnqy6devK19dX7dq1K9brke+ee+5Rv379tHv3bvn5+enDDz9Uenq6FixYYNbp0KGDatasqX79+mnEiBFycnLShx9+KB8fH6Wmptq0d6N9K1++vN544w316dNHbdu2Vbdu3cxHhtWqVUtDhw4t0f7gDme3+6YDt4n8x07s3r27yHq9evUyPDw8rlt29SNHDMMwfvvtN2Po0KFGQECAUb58eePuu+823nzzTZtHRxlG0Y/50DWPqjIMw5g0aZJx1113GY6OjoU+Pmzq1KmGJOOf//xnkftztbZt2xqSzMnT09O4++67jWeffdZYv359oetc+8iwyZMnG/fdd59RsWJFw83Nzahfv74xZcoUIycnx6xz5coVY/DgwYaPj4/h4OBgPgYk/9ElhT2q5HqPDPPw8DCOHj1qdOjQwXB3dzf8/PyMcePGGbm5uTbrnzlzxujatavh7u5uVKpUyXjhhReMAwcOFGjzen0zjMJ/Dt9++60RGRlpeHp6Gu7u7saDDz5o7Nixw6bO9d5b13uUGQCgoI8//tho1aqV4eHhYbi4uBj169c3JkyYYFy6dMmmXv7v1mXLlhXazpIlS4z69esbLi4uRsOGDY3PP//c6Nq1q1G/fv0Cdd977z0jNDTUcHNzMypUqGA0atTIGDlypHHy5EmzTmBgYKGPymzbtm2BR1f98ssvxqBBg4y77rrLcHZ2NqpXr2706tXL+Pnnnwus/8gjjxiSCowpRbl6DHd0dDQqVqxoNGvWzHj55ZcLfSTptWPrzz//bERHRxv169c3PDw8DG9vb6Nly5bGp59+arNeWlqaERUVZVSoUMGQZO5nUd+lrvfIsKioKGPdunVG48aNzZ9rYT+7xMREo2XLloazs7NRs2ZNY/r06YW2eb2+XW/MXbp0qdGsWTPDxcXFqFy5stG9e3fzMbH5rvfd73qPMsOdy8EwuFMPcCeZNWuWhg4dquPHjxd6V1UAAPC7pk2bysfHR3Fxcfbuiumxxx7T/v37C70GHUDZxDXdwB3EMAx98MEHatu2LYEbAID/7/Lly7py5YrNsi1btmjv3r0KDw+3T6cKcerUKa1evbpUbqAG4Nbhmm7gDnD+/Hl9/vnn2rx5s/bv36+VK1fau0sAAJQZP/30kyIiIvTss88qICBA3333nebNmyd/f38NGDDA3t3TsWPHtH37ds2fP1/ly5fXCy+8YO8uASgGQjdwBzhz5oyeeeYZVaxYUf/4xz/017/+1d5dAgCgzKhUqZJCQ0M1f/58nTlzRh4eHoqKitLrr7+uKlWq2Lt7io+PV58+fVSzZk0tXLjQvHs3gNsD13QDAAAAAGARrukGAAAAAMAihG4AAAAAACzCNd03IC8vTydPnlSFChXk4OBg7+4AAP7EDMPQb7/9poCAADk68rfx4mLMBgDcKjc6ZhO6b8DJkydVo0YNe3cDAHAHOXHihKpXr27vbtx2GLMBALfaH43ZhO4bUKFCBUm/v5heXl527g0A4M8sKytLNWrUMMceFA9jNgDgVrnRMZvQfQPyT0/z8vJiAAcA3BKcGl0yjNkAgFvtj8ZsLhYDAAAAAMAihG4AAAAAACxC6AYAAAAAwCKEbgAAAAAALELoBgAAAADAIoRuAAAAAAAsQugGAAAAAMAihG4AAAAAACxC6AYAAAAAwCKEbgAAAAAALELoBgAAAADAIoRuAAAAAAAsQugGAAAAAMAihG4AAAAAACxC6AYAAAAAwCKEbgAAAAAALELoBgAAAADAIoRuAAAAAAAsQugGAAAAAMAihG4AAAAAACxC6AYAAAAAwCLl7N0BAFLoiI/s3QWUQOKbPe3dBQAAyiy+36Cssdd3N450AwAAAABgEUI3AAAAAAAWIXQDAAAAAGARQjcAAAAAABYhdAMAAAAAYBFCNwAAAAAAFrFr6J47d64aN24sLy8veXl5KSwsTF9++aVZfunSJUVHR6tKlSry9PRU165dlZ6ebtNGamqqoqKi5O7uLl9fX40YMUJXrlyxqbNlyxY1b95cLi4uqlu3rmJjY2/F7gEAAAAA7nB2Dd3Vq1fX66+/rsTERH3zzTdq166dOnfurOTkZEnS0KFD9cUXX2jZsmWKj4/XyZMn9fjjj5vr5+bmKioqSjk5OdqxY4cWLlyo2NhYjR071qxz7NgxRUVF6cEHH1RSUpKGDBmi/v37a926dbd8fwEAAAAAdxYHwzAMe3fiapUrV9abb76pv/3tb/Lx8dHixYv1t7/9TZL03XffKTg4WAkJCWrVqpW+/PJLPfroozp58qT8/PwkSfPmzdOoUaN05swZOTs7a9SoUVq9erUOHDhgbuPpp59WRkaG1q5de0N9ysrKkre3tzIzM+Xl5VX6O407XuiIj+zdBZRA4ps97d0F/Akx5twcXj+g7OD7Dcqa0v7udqNjTpm5pjs3N1dLlizR+fPnFRYWpsTERF2+fFkRERFmnfr166tmzZpKSEiQJCUkJKhRo0Zm4JakyMhIZWVlmUfLExISbNrIr5PfBgAAAAAAViln7w7s379fYWFhunTpkjw9PbV8+XKFhIQoKSlJzs7Oqlixok19Pz8/paWlSZLS0tJsAnd+eX5ZUXWysrJ08eJFubm5FehTdna2srOzzfmsrKyb3k8AAAAAwJ3H7ke669Wrp6SkJH399dcaOHCgevXqpYMHD9q1TzExMfL29janGjVq2LU/AAAAAIDbk91Dt7Ozs+rWravQ0FDFxMSoSZMmmjVrlvz9/ZWTk6OMjAyb+unp6fL395ck+fv7F7ibef78H9Xx8vIq9Ci3JI0ePVqZmZnmdOLEidLYVQAAAADAHcbuoftaeXl5ys7OVmhoqMqXL6+NGzeaZSkpKUpNTVVYWJgkKSwsTPv379fp06fNOnFxcfLy8lJISIhZ5+o28uvkt1EYFxcX8zFm+RMAAAAAAMVl12u6R48erY4dO6pmzZr67bfftHjxYm3ZskXr1q2Tt7e3+vXrp2HDhqly5cry8vLS4MGDFRYWplatWkmSOnTooJCQEPXo0UNTp05VWlqaxowZo+joaLm4uEiSBgwYoHfffVcjR45U3759tWnTJn366adavXq1PXcdAAAAAHAHsGvoPn36tHr27KlTp07J29tbjRs31rp16/TQQw9JkmbMmCFHR0d17dpV2dnZioyM1Jw5c8z1nZyctGrVKg0cOFBhYWHy8PBQr169NHHiRLNOUFCQVq9eraFDh2rWrFmqXr265s+fr8jIyFuyjzwq4fbEo6AAAAAAlAa7hu4PPvigyHJXV1fNnj1bs2fPvm6dwMBArVmzpsh2wsPDtWfPnhL1EQAAAACAkipz13QDAAAAAPBnQegGAAAAAMAihG4AAAAAACxC6AYAAAAAwCKEbgAAAAAALELoBgAAAADAIoRuAAAAAAAsQugGAAAAAMAihG4AAAAAACxC6AYAAAAAwCKEbgAAAAAALELoBgAAAADAIoRuAAAAAAAsQugGAAAAAMAihG4AAAAAACxC6AYAAAAAwCKEbgAAAAAALELoBgAAAADAIoRuAAAAAAAsQugGAAAAAMAihG4AAAAAACxC6AYAAAAAwCKEbgAAAAAALELoBgAAAADAIoRuAAAAAAAsQugGAAAAAMAihG4AAAAAACxC6AYAAAAAwCKEbgAAAAAALELoBgAAAADAIoRuAAAAAAAsQugGAAAAAMAihG4AAAAAACxC6AYAAAAAwCKEbgAAAAAALELoBgAARRo/frwcHBxspvr165vlly5dUnR0tKpUqSJPT0917dpV6enpNm2kpqYqKipK7u7u8vX11YgRI3TlyhWbOlu2bFHz5s3l4uKiunXrKjY29lbsHgAAliJ0AwCAP9SgQQOdOnXKnLZt22aWDR06VF988YWWLVum+Ph4nTx5Uo8//rhZnpubq6ioKOXk5GjHjh1auHChYmNjNXbsWLPOsWPHFBUVpQcffFBJSUkaMmSI+vfvr3Xr1t3S/QQAoLSVs3cHAABA2VeuXDn5+/sXWJ6ZmakPPvhAixcvVrt27SRJCxYsUHBwsHbu3KlWrVpp/fr1OnjwoDZs2CA/Pz81bdpUkyZN0qhRozR+/Hg5Oztr3rx5CgoK0rRp0yRJwcHB2rZtm2bMmKHIyMhbuq8AAJQmjnQDAIA/dPjwYQUEBKh27drq3r27UlNTJUmJiYm6fPmyIiIizLr169dXzZo1lZCQIElKSEhQo0aN5OfnZ9aJjIxUVlaWkpOTzTpXt5FfJ78NAABuVxzpBgAARWrZsqViY2NVr149nTp1ShMmTNADDzygAwcOKC0tTc7OzqpYsaLNOn5+fkpLS5MkpaWl2QTu/PL8sqLqZGVl6eLFi3Jzcyu0b9nZ2crOzjbns7KybmpfAQAobYRuAABQpI4dO5r/b9y4sVq2bKnAwEB9+umn1w3Dt0pMTIwmTJhg1z4AAFAUTi8HAADFUrFiRd1zzz06cuSI/P39lZOTo4yMDJs66enp5jXg/v7+Be5mnj//R3W8vLyKDPajR49WZmamOZ04ceJmdw8AgFJF6AYAAMVy7tw5HT16VNWqVVNoaKjKly+vjRs3muUpKSlKTU1VWFiYJCksLEz79+/X6dOnzTpxcXHy8vJSSEiIWefqNvLr5LdxPS4uLvLy8rKZAAAoSwjdAACgSK+88ori4+N1/Phx7dixQ4899picnJzUrVs3eXt7q1+/fho2bJg2b96sxMRE9enTR2FhYWrVqpUkqUOHDgoJCVGPHj20d+9erVu3TmPGjFF0dLRcXFwkSQMGDNAPP/ygkSNH6rvvvtOcOXP06aefaujQofbcdQAAbhrXdAMAgCL9+OOP6tatm3755Rf5+PiodevW2rlzp3x8fCRJM2bMkKOjo7p27ars7GxFRkZqzpw55vpOTk5atWqVBg4cqLCwMHl4eKhXr16aOHGiWScoKEirV6/W0KFDNWvWLFWvXl3z58/ncWEAgNseoRsAABRpyZIlRZa7urpq9uzZmj179nXrBAYGas2aNUW2Ex4erj179pSojwAAlFWcXg4AAAAAgEUI3QAAAAAAWITQDQAAAACARQjdAAAAAABYhNANAAAAAIBFCN0AAAAAAFiE0A0AAAAAgEUI3QAAAAAAWITQDQAAAACARQjdAAAAAABYpJy9OwAAuDGhIz6ydxdQAolv9rR3FwAAgB1xpBsAAAAAAIsQugEAAAAAsAihGwAAAAAAixC6AQAAAACwCKEbAAAAAACLELoBAAAAALAIoRsAAAAAAIsQugEAAAAAsAihGwAAAAAAixC6AQAAAACwCKEbAAAAAACL2DV0x8TE6N5771WFChXk6+urLl26KCUlxaZOeHi4HBwcbKYBAwbY1ElNTVVUVJTc3d3l6+urESNG6MqVKzZ1tmzZoubNm8vFxUV169ZVbGys1bsHAAAAALjD2TV0x8fHKzo6Wjt37lRcXJwuX76sDh066Pz58zb1nnvuOZ06dcqcpk6dapbl5uYqKipKOTk52rFjhxYuXKjY2FiNHTvWrHPs2DFFRUXpwQcfVFJSkoYMGaL+/ftr3bp1t2xfAQAAAAB3nnL23PjatWtt5mNjY+Xr66vExES1adPGXO7u7i5/f/9C21i/fr0OHjyoDRs2yM/PT02bNtWkSZM0atQojR8/Xs7Ozpo3b56CgoI0bdo0SVJwcLC2bdumGTNmKDIy0rodBAAAAADc0crUNd2ZmZmSpMqVK9ssX7RokapWraqGDRtq9OjRunDhglmWkJCgRo0ayc/Pz1wWGRmprKwsJScnm3UiIiJs2oyMjFRCQkKh/cjOzlZWVpbNBAAAAABAcdn1SPfV8vLyNGTIEN1///1q2LChufyZZ55RYGCgAgICtG/fPo0aNUopKSn67LPPJElpaWk2gVuSOZ+WllZknaysLF28eFFubm42ZTExMZowYUKp7yMAAAAA4M5SZkJ3dHS0Dhw4oG3bttksf/75583/N2rUSNWqVVP79u119OhR1alTx5K+jB49WsOGDTPns7KyVKNGDUu2BQAAAAD48yoTp5cPGjRIq1at0ubNm1W9evUi67Zs2VKSdOTIEUmSv7+/0tPTberkz+dfB369Ol5eXgWOckuSi4uLvLy8bCYAAAAAAIrLrqHbMAwNGjRIy5cv16ZNmxQUFPSH6yQlJUmSqlWrJkkKCwvT/v37dfr0abNOXFycvLy8FBISYtbZuHGjTTtxcXEKCwsrpT0BAAAAAKAgu4bu6Ohoffzxx1q8eLEqVKigtLQ0paWl6eLFi5Kko0ePatKkSUpMTNTx48f1+eefq2fPnmrTpo0aN24sSerQoYNCQkLUo0cP7d27V+vWrdOYMWMUHR0tFxcXSdKAAQP0ww8/aOTIkfruu+80Z84cffrppxo6dKjd9h0AAAAA8Odn19A9d+5cZWZmKjw8XNWqVTOnpUuXSpKcnZ21YcMGdejQQfXr19fw4cPVtWtXffHFF2YbTk5OWrVqlZycnBQWFqZnn31WPXv21MSJE806QUFBWr16teLi4tSkSRNNmzZN8+fP53FhAAAAAABL2fVGaoZhFFleo0YNxcfH/2E7gYGBWrNmTZF1wsPDtWfPnmL1DwAAAACAm1EmbqQGAAAAAMCfEaEbAAAAAACLELoBAAAAALAIoRsAAAAAAIsQugEAAAAAsAihGwAAAAAAixC6AQAAAACwCKEbAAAAAACLELoBAAAAALAIoRsAAAAAAIsQugEAAAAAsAihGwAAAAAAixC6AQAAAACwCKEbAAAAAACLELoBAAAAALAIoRsAAAAAAIsQugEAAAAAsAihGwAAAAAAixC6AQAAAACwCKEbAAAAAACLELoBAAAAALAIoRsAAAAAAIsQugEAAAAAsAihGwAAAAAAixC6AQAAAACwCKEbAAAAAACLELoBAAAAALAIoRsAAAAAAIsQugEAAAAAsAihGwAAAAAAixC6AQAAAACwCKEbAAAAAACLELoBAAAAALAIoRsAAAAAAIsQugEAAAAAsAihGwAAAAAAixC6AQAAAACwCKEbAAAAAACLELoBAAAAALAIoRsAAAAAAIsQugEAAAAAsAihGwAAFMvrr78uBwcHDRkyxFx26dIlRUdHq0qVKvL09FTXrl2Vnp5us15qaqqioqLk7u4uX19fjRgxQleuXLGps2XLFjVv3lwuLi6qW7euYmNjb8EeAQBgHUI3AAC4Ybt379a//vUvNW7c2Gb50KFD9cUXX2jZsmWKj4/XyZMn9fjjj5vlubm5ioqKUk5Ojnbs2KGFCxcqNjZWY8eONescO3ZMUVFRevDBB5WUlKQhQ4aof//+Wrdu3S3bPwAAShuhGwAA3JBz586pe/fuev/991WpUiVzeWZmpj744ANNnz5d7dq1U2hoqBYsWKAdO3Zo586dkqT169fr4MGD+vjjj9W0aVN17NhRkyZN0uzZs5WTkyNJmjdvnoKCgjRt2jQFBwdr0KBB+tvf/qYZM2bYZX8BACgNhG4AAHBDoqOjFRUVpYiICJvliYmJunz5ss3y+vXrq2bNmkpISJAkJSQkqFGjRvLz8zPrREZGKisrS8nJyWada9uOjIw02yhMdna2srKybCYAAMqScvbuAAAAKPuWLFmib7/9Vrt37y5QlpaWJmdnZ1WsWNFmuZ+fn9LS0sw6Vwfu/PL8sqLqZGVl6eLFi3Jzcyuw7ZiYGE2YMKHE+wUAgNU40g0AAIp04sQJvfzyy1q0aJFcXV3t3R0bo0ePVmZmpjmdOHHC3l0CAMAGoRsAABQpMTFRp0+fVvPmzVWuXDmVK1dO8fHxevvtt1WuXDn5+fkpJydHGRkZNuulp6fL399fkuTv71/gbub5839Ux8vLq9Cj3JLk4uIiLy8vmwkAgLKE0A0AAIrUvn177d+/X0lJSebUokULde/e3fx/+fLltXHjRnOdlJQUpaamKiwsTJIUFham/fv36/Tp02aduLg4eXl5KSQkxKxzdRv5dfLbAADgdsQ13QAAoEgVKlRQw4YNbZZ5eHioSpUq5vJ+/fpp2LBhqly5sry8vDR48GCFhYWpVatWkqQOHTooJCREPXr00NSpU5WWlqYxY8YoOjpaLi4ukqQBAwbo3Xff1ciRI9W3b19t2rRJn376qVavXn1rdxgAgFJE6AYAADdtxowZcnR0VNeuXZWdna3IyEjNmTPHLHdyctKqVas0cOBAhYWFycPDQ7169dLEiRPNOkFBQVq9erWGDh2qWbNmqXr16po/f74iIyPtsUsAAJQKQjcAACi2LVu22My7urpq9uzZmj179nXXCQwM1Jo1a4psNzw8XHv27CmNLgIAUCZwTTcAAAAAABYhdAMAAAAAYBFCNwAAAAAAFiF0AwAAAABgEUI3AAAAAAAWIXQDAAAAAGARQjcAAAAAABYhdAMAAAAAYBFCNwAAAAAAFiF0AwAAAABgEUI3AAAAAAAWIXQDAAAAAGARQjcAAAAAABYhdAMAAAAAYBFCNwAAAAAAFrFr6I6JidG9996rChUqyNfXV126dFFKSopNnUuXLik6OlpVqlSRp6enunbtqvT0dJs6qampioqKkru7u3x9fTVixAhduXLFps6WLVvUvHlzubi4qG7duoqNjbV69wAAAAAAdzi7hu74+HhFR0dr586diouL0+XLl9WhQwedP3/erDN06FB98cUXWrZsmeLj43Xy5Ek9/vjjZnlubq6ioqKUk5OjHTt2aOHChYqNjdXYsWPNOseOHVNUVJQefPBBJSUlaciQIerfv7/WrVt3S/cXAAAAAHBnKWfPja9du9ZmPjY2Vr6+vkpMTFSbNm2UmZmpDz74QIsXL1a7du0kSQsWLFBwcLB27typVq1aaf369Tp48KA2bNggPz8/NW3aVJMmTdKoUaM0fvx4OTs7a968eQoKCtK0adMkScHBwdq2bZtmzJihyMjIW77fAAAAAIA7Q5m6pjszM1OSVLlyZUlSYmKiLl++rIiICLNO/fr1VbNmTSUkJEiSEhIS1KhRI/n5+Zl1IiMjlZWVpeTkZLPO1W3k18lv41rZ2dnKysqymQAAAAAAKK4yE7rz8vI0ZMgQ3X///WrYsKEkKS0tTc7OzqpYsaJNXT8/P6WlpZl1rg7c+eX5ZUXVycrK0sWLFwv0JSYmRt7e3uZUo0aNUtlHAAAAAMCdpcyE7ujoaB04cEBLliyxd1c0evRoZWZmmtOJEyfs3SUAAAAAwG3Irtd05xs0aJBWrVqlrVu3qnr16uZyf39/5eTkKCMjw+Zod3p6uvz9/c06u3btsmkv/+7mV9e59o7n6enp8vLykpubW4H+uLi4yMXFpVT2DQAAAABw57LrkW7DMDRo0CAtX75cmzZtUlBQkE15aGioypcvr40bN5rLUlJSlJqaqrCwMElSWFiY9u/fr9OnT5t14uLi5OXlpZCQELPO1W3k18lvAwAAAAAAK9j1SHd0dLQWL16slStXqkKFCuY12N7e3nJzc5O3t7f69eunYcOGqXLlyvLy8tLgwYMVFhamVq1aSZI6dOigkJAQ9ejRQ1OnTlVaWprGjBmj6Oho82j1gAED9O6772rkyJHq27evNm3apE8//VSrV6+2274DAAAAAP787Hqke+7cucrMzFR4eLiqVatmTkuXLjXrzJgxQ48++qi6du2qNm3ayN/fX5999plZ7uTkpFWrVsnJyUlhYWF69tln1bNnT02cONGsExQUpNWrVysuLk5NmjTRtGnTNH/+fB4XBgAAAACwlF2PdBuG8Yd1XF1dNXv2bM2ePfu6dQIDA7VmzZoi2wkPD9eePXuK3UcAAAAAAEqqzNy9HAAAAACAPxtCNwAAAAAAFiF0AwAAAABgEUI3AAAAAAAWIXQDAAAAAGARQjcAAAAAABYhdAMAAAAAYBFCNwAAAAAAFiF0AwAAAABgEUI3AAAAAAAWIXQDAAAAAGARQjcAAAAAABYhdAMAAAAAYBFCNwAAAAAAFiF0AwAAAABgEUI3AAAAAAAWIXQDAAAAAGARQjcAAAAAABYhdAMAAAAAYBFCNwAAAAAAFiF0AwAAAABgEUI3AAAAAAAWIXQDAAAAAGARQjcAAAAAABYhdAMAAAAAYBFCNwAAAAAAFiF0AwAAAABgEUI3AAAAAAAWIXQDAAAAAGARQjcAAAAAABYhdAMAAAAAYBFCNwAAAAAAFiF0AwAAAABgEUI3AAAAAAAWIXQDAAAAAGARQjcAAAAAABYpUehu166dMjIyCizPyspSu3btbrZPAACgFDBeAwBgfyUK3Vu2bFFOTk6B5ZcuXdJXX311050CAAA3j/EaAAD7K1ecyvv27TP/f/DgQaWlpZnzubm5Wrt2re66667S6x0AACg2xmsAAMqOYoXupk2bysHBQQ4ODoWelubm5qZ33nmn1DoHAACKj/EaAICyo1ih+9ixYzIMQ7Vr19auXbvk4+Njljk7O8vX11dOTk6l3kkAAHDjGK8BACg7ihW6AwMDJUl5eXmWdAYAANw8xmsAAMqOYoXuqx0+fFibN2/W6dOnCwzqY8eOvemOAQCAm8d4DQCAfZUodL///vsaOHCgqlatKn9/fzk4OJhlDg4ODOIAAJQBjNcAANhfiUL35MmTNWXKFI0aNaq0+wMAAEoJ4zUAAPZXoud0nz17Vk888URp9wUAAJQixmsAAOyvRKH7iSee0Pr160u7LwAAoBQxXgMAYH8lOr28bt26eu2117Rz5041atRI5cuXtyl/6aWXSqVzAACg5EprvJ47d67mzp2r48ePS5IaNGigsWPHqmPHjpKkS5cuafjw4VqyZImys7MVGRmpOXPmyM/Pz2wjNTVVAwcO1ObNm+Xp6alevXopJiZG5cr976vIli1bNGzYMCUnJ6tGjRoaM2aMevfufXMvAgAAdlai0P3ee+/J09NT8fHxio+PtylzcHAgdAMAUAaU1nhdvXp1vf7667r77rtlGIYWLlyozp07a8+ePWrQoIGGDh2q1atXa9myZfL29tagQYP0+OOPa/v27ZKk3NxcRUVFyd/fXzt27NCpU6fUs2dPlS9fXv/85z8l/f5s8aioKA0YMECLFi3Sxo0b1b9/f1WrVk2RkZGl+8IAAHALlSh0Hzt2rLT7AQAASllpjdedOnWymZ8yZYrmzp2rnTt3qnr16vrggw+0ePFitWvXTpK0YMECBQcHa+fOnWrVqpXWr1+vgwcPasOGDfLz81PTpk01adIkjRo1SuPHj5ezs7PmzZunoKAgTZs2TZIUHBysbdu2acaMGYRuAMBtrUTXdAMAgDtTbm6ulixZovPnzyssLEyJiYm6fPmyIiIizDr169dXzZo1lZCQIElKSEhQo0aNbE43j4yMVFZWlpKTk806V7eRXye/DQAAblclOtLdt2/fIss//PDDEnUGAACUntIcr/fv36+wsDBdunRJnp6eWr58uUJCQpSUlCRnZ2dVrFjRpr6fn5/S0tIkSWlpaTaBO788v6yoOllZWbp48aLc3NwK7Vd2drays7PN+aysrBveJwAAboUShe6zZ8/azF++fFkHDhxQRkaGeWoZAACwr9Icr+vVq6ekpCRlZmbqP//5j3r16lXgOnF7iImJ0YQJE+zdDQAArqtEoXv58uUFluXl5WngwIGqU6fOTXcKAADcvNIcr52dnVW3bl1JUmhoqHbv3q1Zs2bpqaeeUk5OjjIyMmyOdqenp8vf31+S5O/vr127dtm0l56ebpbl/5u/7Oo6Xl5e1z3KLUmjR4/WsGHDzPmsrCzVqFGjWPsGAICVShS6C+Po6Khhw4YpPDxcI0eOLK1mAQBAKSqt8TovL0/Z2dkKDQ1V+fLltXHjRnXt2lWSlJKSotTUVIWFhUmSwsLCNGXKFJ0+fVq+vr6SpLi4OHl5eSkkJMSss2bNGpttxMXFmW1cj4uLi1xcXEq8HzcqdMRHlm8DKI7EN3vauwsAblCphW5JOnr0qK5cuVKaTQIAgFJW3PF69OjR6tixo2rWrKnffvtNixcv1pYtW7Ru3Tp5e3urX79+GjZsmCpXriwvLy8NHjxYYWFhatWqlSSpQ4cOCgkJUY8ePTR16lSlpaVpzJgxio6ONgPzgAED9O6772rkyJHq27evNm3apE8//VSrV6+25DUAAOBWKVHovvo0LkkyDEOnTp3S6tWr1atXr1LpGAAAuDmlNV6fPn1aPXv21KlTp+Tt7a3GjRtr3bp1euihhyRJM2bMkKOjo7p27ars7GxFRkZqzpw55vpOTk5atWqVBg4cqLCwMHl4eKhXr16aOHGiWScoKEirV6/W0KFDNWvWLFWvXl3z58/ncWEAgNteiUL3nj17bOYdHR3l4+OjadOm/eGdUgEAwK1RWuP1Bx98UGS5q6urZs+erdmzZ1+3TmBgYIHTx68VHh5eoM8AANzuShS6N2/eXNr9AAAApYzxGgAA+7upa7rPnDmjlJQUSb8/SsTHx6dUOgUAAEoP4zUAAPbjWJKVzp8/r759+6patWpq06aN2rRpo4CAAPXr108XLlwo7T4CAIASYLwGAMD+ShS6hw0bpvj4eH3xxRfKyMhQRkaGVq5cqfj4eA0fPry0+wgAAEqA8RoAAPsr0enl//3vf/Wf//xH4eHh5rJHHnlEbm5uevLJJzV37tzS6h8AACghxmsAAOyvREe6L1y4ID8/vwLLfX19OV0NAIAygvEaAAD7K1HoDgsL07hx43Tp0iVz2cWLFzVhwgSFhYWVWucAAEDJMV4DAGB/JTq9fObMmXr44YdVvXp1NWnSRJK0d+9eubi4aP369aXaQQAAUDKM1wAA2F+JQnejRo10+PBhLVq0SN99950kqVu3burevbvc3NxKtYMAAKBkGK8BALC/EoXumJgY+fn56bnnnrNZ/uGHH+rMmTMaNWrUDbWzdetWvfnmm0pMTNSpU6e0fPlydenSxSzv3bu3Fi5caLNOZGSk1q5da87/+uuvGjx4sL744gs5Ojqqa9eumjVrljw9Pc06+/btU3R0tHbv3i0fHx8NHjxYI0eOLMGeAwBw+yit8RoAAJRcia7p/te//qX69esXWN6gQQPNmzfvhts5f/68mjRpotmzZ1+3zsMPP6xTp06Z0yeffGJT3r17dyUnJysuLk6rVq3S1q1b9fzzz5vlWVlZ6tChgwIDA5WYmKg333xT48eP13vvvXfD/QQA4HZUWuM1AAAouRId6U5LS1O1atUKLPfx8dGpU6duuJ2OHTuqY8eORdZxcXGRv79/oWWHDh3S2rVrtXv3brVo0UKS9M477+iRRx7RW2+9pYCAAC1atEg5OTn68MMP5ezsrAYNGigpKUnTp0+3CecAAPzZlNZ4DQAASq5ER7pr1Kih7du3F1i+fft2BQQE3HSnrrZlyxb5+vqqXr16GjhwoH755RezLCEhQRUrVjQDtyRFRETI0dFRX3/9tVmnTZs2cnZ2NutERkYqJSVFZ8+eLXSb2dnZysrKspkAALjd3MrxGgAAFK5ER7qfe+45DRkyRJcvX1a7du0kSRs3btTIkSM1fPjwUuvcww8/rMcff1xBQUE6evSo/vGPf6hjx45KSEiQk5OT0tLS5Ovra7NOuXLlVLlyZaWlpUn6/a/8QUFBNnXyn1malpamSpUqFdhuTEyMJkyYUGr7AQCAPdyq8RoAAFxfiUL3iBEj9Msvv+jFF19UTk6OJMnV1VWjRo3S6NGjS61zTz/9tPn/Ro0aqXHjxqpTp462bNmi9u3bl9p2rjV69GgNGzbMnM/KylKNGjUs2x4AAFa4VeM1AAC4vhKFbgcHB73xxht67bXXdOjQIbm5uenuu++Wi4tLaffPRu3atVW1alUdOXJE7du3l7+/v06fPm1T58qVK/r111/N68D9/f2Vnp5uUyd//nrXiru4uFi+LwAAWM1e4zUAAPifEl3Tnc/T01P33nuvGjZseEsG8B9//FG//PKLeVOYsLAwZWRkKDEx0ayzadMm5eXlqWXLlmadrVu36vLly2aduLg41atXr9BTywEA+LO51eM1AAD4n5sK3Tfr3LlzSkpKUlJSkiTp2LFjSkpKUmpqqs6dO6cRI0Zo586dOn78uDZu3KjOnTurbt26ioyMlCQFBwfr4Ycf1nPPPaddu3Zp+/btGjRokJ5++mnzBjHPPPOMnJ2d1a9fPyUnJ2vp0qWaNWuWzenjAAAAAABYwa6h+5tvvlGzZs3UrFkzSdKwYcPUrFkzjR07Vk5OTtq3b5/++te/6p577lG/fv0UGhqqr776yuav9IsWLVL9+vXVvn17PfLII2rdurXNM7i9vb21fv16HTt2TKGhoRo+fLjGjh3L48IAAAAAAJYr0TXdpSU8PFyGYVy3fN26dX/YRuXKlbV48eIi6zRu3FhfffVVsfsHAAAAAMDNsOuRbgAAAAAA/swI3QAAAAAAWITQDQAAAACARQjdAAAAAABYhNANAAAAAIBFCN0AAAAAAFiE0A0AAAAAgEUI3QAAAAAAWITQDQAAAACARQjdAAAAAABYhNANAAAAAIBFCN0AAAAAAFiE0A0AAAAAgEUI3QAAAAAAWITQDQAAAACARQjdAAAAAABYhNANAAAAAIBFCN0AAAAAAFiE0A0AAAAAgEUI3QAAAAAAWITQDQAAAACARQjdAAAAAABYhNANAAAAAIBFCN0AAAAAAFiE0A0AAAAAgEUI3QAAAAAAWITQDQAAAACARQjdAAAAAABYhNANAAAAAIBFCN0AAAAAAFiE0A0AAAAAgEUI3QAAAAAAWITQDQAAAACARQjdAAAAAABYhNANAAAAAIBFCN0AAAAAAFiE0A0AAAAAgEUI3QAAAAAAWITQDQAAAACARQjdAAAAAABYhNANAAAAAIBFCN0AAAAAAFiE0A0AAAAAgEUI3QAAAAAAWITQDQAAAACARQjdAAAAAABYhNANAAAAAIBFCN0AAAAAAFiE0A0AAAAAgEUI3QAAAAAAWITQDQAAAACARQjdAAAAAABYhNANAAAAAIBFCN0AAAAAAFiE0A0AAIoUExOje++9VxUqVJCvr6+6dOmilJQUmzqXLl1SdHS0qlSpIk9PT3Xt2lXp6ek2dVJTUxUVFSV3d3f5+vpqxIgRunLlik2dLVu2qHnz5nJxcVHdunUVGxtr9e4BAGApQjcAAChSfHy8oqOjtXPnTsXFxeny5cvq0KGDzp8/b9YZOnSovvjiCy1btkzx8fE6efKkHn/8cbM8NzdXUVFRysnJ0Y4dO7Rw4ULFxsZq7NixZp1jx44pKipKDz74oJKSkjRkyBD1799f69atu6X7CwBAaSpn7w4AAICybe3atTbzsbGx8vX1VWJiotq0aaPMzEx98MEHWrx4sdq1aydJWrBggYKDg7Vz5061atVK69ev18GDB7Vhwwb5+fmpadOmmjRpkkaNGqXx48fL2dlZ8+bNU1BQkKZNmyZJCg4O1rZt2zRjxgxFRkbe8v0GAKA0cKQbAAAUS2ZmpiSpcuXKkqTExERdvnxZERERZp369eurZs2aSkhIkCQlJCSoUaNG8vPzM+tERkYqKytLycnJZp2r28ivk99GYbKzs5WVlWUzAQBQlhC6AQDADcvLy9OQIUN0//33q2HDhpKktLQ0OTs7q2LFijZ1/fz8lJaWZta5OnDnl+eXFVUnKytLFy9eLLQ/MTEx8vb2NqcaNWrc9D4CAFCaCN0AAOCGRUdH68CBA1qyZIm9uyJJGj16tDIzM83pxIkT9u4SAAA2uKYbAADckEGDBmnVqlXaunWrqlevbi739/dXTk6OMjIybI52p6eny9/f36yza9cum/by725+dZ1r73ienp4uLy8vubm5FdonFxcXubi43PS+AQBgFY50AwCAIhmGoUGDBmn58uXatGmTgoKCbMpDQ0NVvnx5bdy40VyWkpKi1NRUhYWFSZLCwsK0f/9+nT592qwTFxcnLy8vhYSEmHWubiO/Tn4bAADcjjjSDQAAihQdHa3Fixdr5cqVqlChgnkNtre3t9zc3OTt7a1+/fpp2LBhqly5sry8vDR48GCFhYWpVatWkqQOHTooJCREPXr00NSpU5WWlqYxY8YoOjraPFI9YMAAvfvuuxo5cqT69u2rTZs26dNPP9Xq1avttu8AANwsjnQDAIAizZ07V5mZmQoPD1e1atXMaenSpWadGTNm6NFHH1XXrl3Vpk0b+fv767PPPjPLnZyctGrVKjk5OSksLEzPPvusevbsqYkTJ5p1goKCtHr1asXFxalJkyaaNm2a5s+fz+PCAAC3NY50AwCAIhmG8Yd1XF1dNXv2bM2ePfu6dQIDA7VmzZoi2wkPD9eePXuK3UcAAMoqjnQDAAAAAGARQjcAAAAAABaxa+jeunWrOnXqpICAADk4OGjFihU25YZhaOzYsapWrZrc3NwUERGhw4cP29T59ddf1b17d3l5ealixYrq16+fzp07Z1Nn3759euCBB+Tq6qoaNWpo6tSpVu8aAAAAAAD2Dd3nz59XkyZNrnv919SpU/X2229r3rx5+vrrr+Xh4aHIyEhdunTJrNO9e3clJycrLi7OfHbo888/b5ZnZWWpQ4cOCgwMVGJiot58802NHz9e7733nuX7BwAAAAC4s9n1RmodO3ZUx44dCy0zDEMzZ87UmDFj1LlzZ0nSRx99JD8/P61YsUJPP/20Dh06pLVr12r37t1q0aKFJOmdd97RI488orfeeksBAQFatGiRcnJy9OGHH8rZ2VkNGjRQUlKSpk+fbhPOAQAAAAAobWX2mu5jx44pLS1NERER5jJvb2+1bNlSCQkJkqSEhARVrFjRDNySFBERIUdHR3399ddmnTZt2sjZ2dmsExkZqZSUFJ09e7bQbWdnZysrK8tmAgAAAACguMps6E5LS5Mk+fn52Sz38/Mzy9LS0uTr62tTXq5cOVWuXNmmTmFtXL2Na8XExMjb29ucatSocfM7BAAAAAC445TZ0G1Po0ePVmZmpjmdOHHC3l0CAAAAANyGymzo9vf3lySlp6fbLE9PTzfL/P39dfr0aZvyK1eu6Ndff7WpU1gbV2/jWi4uLvLy8rKZAAAAAAAorjIbuoOCguTv76+NGzeay7KysvT1118rLCxMkhQWFqaMjAwlJiaadTZt2qS8vDy1bNnSrLN161ZdvnzZrBMXF6d69eqpUqVKt2hvAAAAAAB3IruG7nPnzikpKUlJSUmSfr95WlJSklJTU+Xg4KAhQ4Zo8uTJ+vzzz7V//3717NlTAQEB6tKliyQpODhYDz/8sJ577jnt2rVL27dv16BBg/T0008rICBAkvTMM8/I2dlZ/fr1U3JyspYuXapZs2Zp2LBhdtprAAAAAMCdwq6PDPvmm2/04IMPmvP5QbhXr16KjY3VyJEjdf78eT3//PPKyMhQ69attXbtWrm6uprrLFq0SIMGDVL79u3l6Oiorl276u233zbLvb29tX79ekVHRys0NFRVq1bV2LFjeVwYAAAAAMBydg3d4eHhMgzjuuUODg6aOHGiJk6ceN06lStX1uLFi4vcTuPGjfXVV1+VuJ8AAAAAAJREmb2mGwAAAACA2x2hGwAAAAAAixC6AQAAAACwCKEbAAAAAACLELoBAAAAALAIoRsAAAAAAIsQugEAAAAAsAihGwAAAAAAixC6AQAAAACwCKEbAAAAAACLELoBAAAAALAIoRsAAAAAAIsQugEAAAAAsAihGwAAAAAAixC6AQAAAACwCKEbAAAAAACLELoBAAAAALAIoRsAAAAAAIsQugEAAAAAsAihGwAAAAAAixC6AQAAAACwCKEbAAAAAACLELoBAAAAALAIoRsAAAAAAIsQugEAAAAAsAihGwAAAAAAixC6AQAAAACwCKEbAAAAAACLELoBAAAAALAIoRsAAAAAAIsQugEAAAAAsAihGwAAAAAAixC6AQAAAACwCKEbAAAAAACLELoBAAAAALAIoRsAAAAAAIsQugEAAAAAsAihGwAAAAAAixC6AQAAAACwCKEbAAAAAACLELoBAAAAALAIoRsAAAAAAIsQugEAAAAAsAihGwAAAAAAixC6AQAAAACwCKEbAAAAAACLELoBAAAAALAIoRsAAAAAAIsQugEAAAAAsAihGwAAAAAAixC6AQAAAACwCKEbAAAAAACLELoBAAAAALAIoRsAAAAAAIsQugEAAAAAsAihGwAAAAAAixC6AQAAAACwCKEbAAAAAACLELoBAAAAALAIoRsAAAAAAIsQugEAAAAAsAihGwAA/KGtW7eqU6dOCggIkIODg1asWGFTbhiGxo4dq2rVqsnNzU0RERE6fPiwTZ1ff/1V3bt3l5eXlypWrKh+/frp3LlzNnX27dunBx54QK6urqpRo4amTp1q9a4BAGApQjcAAPhD58+fV5MmTTR79uxCy6dOnaq3335b8+bN09dffy0PDw9FRkbq0qVLZp3u3bsrOTlZcXFxWrVqlbZu3arnn3/eLM/KylKHDh0UGBioxMREvfnmmxo/frzee+89y/cPAACrlLN3BwAAQNnXsWNHdezYsdAywzA0c+ZMjRkzRp07d5YkffTRR/Lz89OKFSv09NNP69ChQ1q7dq12796tFi1aSJLeeecdPfLII3rrrbcUEBCgRYsWKScnRx9++KGcnZ3VoEEDJSUlafr06TbhHACA2wlHugEAwE05duyY0tLSFBERYS7z9vZWy5YtlZCQIElKSEhQxYoVzcAtSREREXJ0dNTXX39t1mnTpo2cnZ3NOpGRkUpJSdHZs2cL3XZ2draysrJsJgAAyhJCNwAAuClpaWmSJD8/P5vlfn5+ZllaWpp8fX1tysuVK6fKlSvb1Cmsjau3ca2YmBh5e3ubU40aNW5+hwAAKEWEbgAAcNsaPXq0MjMzzenEiRP27hIAADbKdOgeP368HBwcbKb69eub5ZcuXVJ0dLSqVKkiT09Pde3aVenp6TZtpKamKioqSu7u7vL19dWIESN05cqVW70rAAD8afn7+0tSgTE4PT3dLPP399fp06dtyq9cuaJff/3Vpk5hbVy9jWu5uLjIy8vLZgIAoCwp06Fbkho0aKBTp06Z07Zt28yyoUOH6osvvtCyZcsUHx+vkydP6vHHHzfLc3NzFRUVpZycHO3YsUMLFy5UbGysxo4da49dAQDgTykoKEj+/v7auHGjuSwrK0tff/21wsLCJElhYWHKyMhQYmKiWWfTpk3Ky8tTy5YtzTpbt27V5cuXzTpxcXGqV6+eKlWqdIv2BgCA0lXmQ3e5cuXk7+9vTlWrVpUkZWZm6oMPPtD06dPVrl07hYaGasGCBdqxY4d27twpSVq/fr0OHjyojz/+WE2bNlXHjh01adIkzZ49Wzk5OfbcLQAAbivnzp1TUlKSkpKSJP1+87SkpCSlpqbKwcFBQ4YM0eTJk/X5559r//796tmzpwICAtSlSxdJUnBwsB5++GE999xz2rVrl7Zv365Bgwbp6aefVkBAgCTpmWeekbOzs/r166fk5GQtXbpUs2bN0rBhw+y01wAA3LwyH7oPHz6sgIAA1a5dW927d1dqaqokKTExUZcvX7a5U2r9+vVVs2ZNmzulNmrUyOamLJGRkcrKylJycvKt3REAAG5j33zzjZo1a6ZmzZpJkoYNG6ZmzZqZZ4+NHDlSgwcP1vPPP697771X586d09q1a+Xq6mq2sWjRItWvX1/t27fXI488otatW9s8g9vb21vr16/XsWPHFBoaquHDh2vs2LE8LgwAcFsr08/pbtmypWJjY1WvXj2dOnVKEyZM0AMPPKADBw4oLS1Nzs7Oqlixos06194ptbh3QZV+f/xIdna2Oc/jRwAAd7rw8HAZhnHdcgcHB02cOFETJ068bp3KlStr8eLFRW6ncePG+uqrr0rcTwAAypoyHbo7duxo/r9x48Zq2bKlAgMD9emnn8rNzc2y7cbExGjChAmWtQ8AAAAAuDOU+dPLr1axYkXdc889OnLkiPz9/ZWTk6OMjAybOtfeKbW4d0GVePwIAAAAAKB03Fah+9y5czp69KiqVaum0NBQlS9f3uZOqSkpKUpNTbW5U+r+/fttHlESFxcnLy8vhYSEXHc7PH4EAAAAAFAayvTp5a+88oo6deqkwMBAnTx5UuPGjZOTk5O6desmb29v9evXT8OGDVPlypXl5eWlwYMHKywsTK1atZIkdejQQSEhIerRo4emTp2qtLQ0jRkzRtHR0XJxcbHz3gEAAAAA/uzKdOj+8ccf1a1bN/3yyy/y8fFR69attXPnTvn4+EiSZsyYIUdHR3Xt2lXZ2dmKjIzUnDlzzPWdnJy0atUqDRw4UGFhYfLw8FCvXr2KvMkLAAAAAAClpUyH7iVLlhRZ7urqqtmzZ2v27NnXrRMYGKg1a9aUdtcAAAAAAPhDt9U13QAAAAAA3E4I3QAAAAAAWITQDQAAAACARQjdAAAAAABYhNANAAAAAIBFCN0AAAAAAFiE0A0AAAAAgEUI3QAAAAAAWITQDQAAAACARQjdAAAAAABYhNANAAAAAIBFCN0AAAAAAFiE0A0AAAAAgEUI3QAAAAAAWITQDQAAAACARQjdAAAAAABYhNANAAAAAIBFCN0AAAAAAFiE0A0AAAAAgEUI3QAAAAAAWITQDQAAAACARQjdAAAAAABYhNANAAAAAIBFCN0AAAAAAFiE0A0AAAAAgEUI3QAAAAAAWITQDQAAAACARQjdAAAAAABYhNANAAAAAIBFCN0AAAAAAFiE0A0AAAAAgEUI3QAAAAAAWITQDQAAAACARQjdAAAAAABYhNANAAAAAIBFCN0AAAAAAFiE0A0AAAAAgEUI3QAAAAAAWITQDQAAAACARQjdAAAAAABYhNANAAAAAIBFCN0AAAAAAFiE0A0AAAAAgEUI3QAAAAAAWITQDQAAAACARQjdAAAAAABYhNANAAAAAIBFCN0AAAAAAFiE0A0AAAAAgEUI3QAAAAAAWITQDQAAAACARQjdAAAAAABYhNANAAAAAIBFCN0AAAAAAFiE0A0AAAAAgEUI3QAAAAAAWITQDQAAAACARQjdAAAAAABYhNANAAAAAIBFCN0AAAAAAFiE0A0AAAAAgEUI3QAAAAAAWITQDQAAAACARQjdAAAAAABYhNANAAAAAIBFCN0AAAAAAFiE0A0AAAAAgEXuqNA9e/Zs1apVS66urmrZsqV27dpl7y4BAIBrMF4DAP5M7pjQvXTpUg0bNkzjxo3Tt99+qyZNmigyMlKnT5+2d9cAAMD/x3gNAPizuWNC9/Tp0/Xcc8+pT58+CgkJ0bx58+Tu7q4PP/zQ3l0DAAD/H+M1AODP5o4I3Tk5OUpMTFRERIS5zNHRUREREUpISLBjzwAAQD7GawDAn1E5e3fgVvj555+Vm5srPz8/m+V+fn767rvvCtTPzs5Wdna2OZ+ZmSlJysrKKva2c7MvFnsd2F9JftY3g/fJ7Yn3CW5Ecd8n+fUNw7CiO2VaccdrqXTH7KLw+UNZc6vHoJLgc4OyprQ/Nzc6Zt8Robu4YmJiNGHChALLa9SoYYfewB683xlg7y7gNsD7BDeipO+T3377Td7e3qXcmz8fxmzcqRiDgOKz6nPzR2P2HRG6q1atKicnJ6Wnp9ssT09Pl7+/f4H6o0eP1rBhw8z5vLw8/frrr6pSpYocHBws7+/tICsrSzVq1NCJEyfk5eVl7+6gjOJ9ghvB+8SWYRj67bffFBAQYO+u3HLFHa8lxuzbCZ91oGT47JRdNzpm3xGh29nZWaGhodq4caO6dOki6fdBeePGjRo0aFCB+i4uLnJxcbFZVrFixVvQ09uPl5cXH378Id4nuBG8T/7nTj3CXdzxWmLMvh3xWQdKhs9O2XQjY/YdEboladiwYerVq5datGih++67TzNnztT58+fVp08fe3cNAAD8f4zXAIA/mzsmdD/11FM6c+aMxo4dq7S0NDVt2lRr164tcLMWAABgP4zXAIA/mzsmdEvSoEGDrnt6GorHxcVF48aNK3BKH3A13ie4EbxPcC3G6z8nPutAyfDZuf05GHfiM0kAAAAAALgFHO3dAQAAAAAA/qwI3QAAAAAAWITQDYWHh2vIkCHXLXdwcNCKFStuuL0tW7bIwcFBGRkZN903lE1/9J4BbtS176VatWpp5syZdusPAABAaSN04w+dOnVKHTt2tHc3ANwBdu/ereeff97e3QBQhsXGxvIsdpRp48ePV9OmTe3djVLVu3dvdenSxd7duG3dUXcvR8n4+/vbuwsA7hA+Pj727gKAErh8+bLKly9v724AdwzDMJSbm6ty5YhztwOOdEOSlJeXp5EjR6py5cry9/fX+PHjzbJrTy/fsWOHmjZtKldXV7Vo0UIrVqyQg4ODkpKSbNpMTExUixYt5O7urr/85S9KSUm5NTuDW+rs2bPq2bOnKlWqJHd3d3Xs2FGHDx+W9PuA4OPjo//85z9m/aZNm6patWrm/LZt2+Ti4qILFy7c8r7j+sLDwzV48GANGTJElSpVkp+fn95//32dP39effr0UYUKFVS3bl19+eWX5joHDhxQx44d5enpKT8/P/Xo0UM///yzWX7+/Hn17NlTnp6eqlatmqZNm1Zgu1efXn78+PECv1syMjLk4OCgLVu2SPrf5Szr1q1Ts2bN5Obmpnbt2un06dP68ssvFRwcLC8vLz3zzDO8x4BiKOxSj6ZNm5rfDxwcHDR37lz99a9/lYeHh6ZMmSJJmjx5snx9fVWhQgX1799ff//73wsc8Zs/f76Cg4Pl6uqq+vXra86cOWZZ/uf+s88+04MPPih3d3c1adJECQkJkn7/zPfp00eZmZlycHCQg4ODzXcWoDSEh4frpZdeuu5349TUVHXu3Fmenp7y8vLSk08+qfT0dEm/n4kxYcIE7d2713yPxsbGFrm94ox3X375pUJDQ+Xi4qJt27bpt99+U/fu3eXh4aFq1appxowZBS7dys7O1iuvvKK77rpLHh4eatmypdlufp8rVqyodevWKTg4WJ6ennr44Yd16tQpSb8fuV+4cKFWrlxp7tPV6+OPEbohSVq4cKE8PDz09ddfa+rUqZo4caLi4uIK1MvKylKnTp3UqFEjffvtt5o0aZJGjRpVaJuvvvqqpk2bpm+++UblypVT3759rd4N2EHv3r31zTff6PPPP1dCQoIMw9Ajjzyiy5cvy8HBQW3atDF/MZ89e1aHDh3SxYsX9d1330mS4uPjde+998rd3d2Oe4HCLFy4UFWrVtWuXbs0ePBgDRw4UE888YT+8pe/6Ntvv1WHDh3Uo0cPXbhwQRkZGWrXrp2aNWumb775RmvXrlV6erqefPJJs70RI0YoPj5eK1eu1Pr167VlyxZ9++23pdLX8ePH691339WOHTt04sQJPfnkk5o5c6YWL16s1atXa/369XrnnXdKZVsAfjd+/Hg99thj2r9/v/r27atFixZpypQpeuONN5SYmKiaNWtq7ty5NussWrRIY8eO1ZQpU3To0CH985//1GuvvaaFCxfa1Hv11Vf1yiuvKCkpSffcc4+6deumK1eu6C9/+YtmzpwpLy8vnTp1SqdOndIrr7xyK3cbd4jrfTfOy8tT586d9euvvyo+Pl5xcXH64Ycf9NRTT0mSnnrqKQ0fPlwNGjQw36P5ZaXh73//u15//XUdOnRIjRs31rBhw7R9+3Z9/vnniouL01dffVVgbB00aJASEhK0ZMkS7du3T0888YQefvhh8yCJJF24cEFvvfWW/v3vf2vr1q1KTU01P1uvvPKKnnzySTOInzp1Sn/5y19KbZ/uCAbueG3btjVat25ts+zee+81Ro0aZRiGYUgyli9fbhiGYcydO9eoUqWKcfHiRbPu+++/b0gy9uzZYxiGYWzevNmQZGzYsMGss3r1akOSzXq4fbVt29Z4+eWXje+//96QZGzfvt0s+/nnnw03Nzfj008/NQzDMN5++22jQYMGhmEYxooVK4yWLVsanTt3NubOnWsYhmFEREQY//jHP279TqBI1/5euHLliuHh4WH06NHDXHbq1ClDkpGQkGBMmjTJ6NChg00bJ06cMCQZKSkpxm+//WY4Ozub7wvDMIxffvnFcHNzM15++WVzWWBgoDFjxgzDMAzj2LFjNr9bDMMwzp49a0gyNm/ebBhG4b9vYmJiDEnG0aNHzWUvvPCCERkZeTMvCXBHufqzmK9JkybGuHHjDMP4/bvBkCFDbMpbtmxpREdH2yy7//77jSZNmpjzderUMRYvXmxTZ9KkSUZYWJhhGP/73M+fP98sT05ONiQZhw4dMgzDMBYsWGB4e3vfxN4BRSvqu/H69esNJycnIzU11SzLf4/u2rXLMAzDGDdunM37/o8UZ7xbsWKFWScrK8soX768sWzZMnNZRkaG4e7ubo6t//d//2c4OTkZP/30k80227dvb4wePdowjN8/U5KMI0eOmOWzZ882/Pz8zPlevXoZnTt3vuF9gi2OdEOS1LhxY5v5atWq6fTp0wXqpaSkqHHjxnJ1dTWX3XfffX/YZv7pxIW1idvXoUOHVK5cObVs2dJcVqVKFdWrV0+HDh2SJLVt21YHDx7UmTNnFB8fr/DwcIWHh2vLli26fPmyduzYofDwcDvtAYpy9WfYyclJVapUUaNGjcxlfn5+kn7/XO/du1ebN2+Wp6enOdWvX1+SdPToUR09elQ5OTk275XKlSurXr16pd5XPz8/ubu7q3bt2jbL+P0DlK4WLVrYzKekpBT4TnD1/Pnz53X06FH169fP5nfF5MmTdfToUZv1+A4Be7ved+NDhw6pRo0aqlGjhlkWEhKiihUrmt99rHT15+6HH37Q5cuXbT5n3t7eNmPr/v37lZubq3vuucfmcxcfH2/zuXN3d1edOnXM+etlAZQMV95Dkgrc/MTBwUF5eXml1qaDg4Mk3XSbuP00atRIlStXVnx8vOLj4zVlyhT5+/vrjTfe0O7du3X58mVOUSqjCvu9cL3P9blz59SpUye98cYbBdqpVq2ajhw5UuztOzr+/ndhwzDMZZcvX/7Dvl7bz/xl/P4Bbpyjo6PNZ08q+Pnz8PAoVpvnzp2TJL3//vs2f4CTfv/D3tX4DgF7u5XjSHHGu5J87pycnJSYmFjgc+bp6Wn+v7D9vfZ3AEqOI90olnr16mn//v3Kzs42l+3evduOPYI9BQcH68qVK/r666/NZb/88otSUlIUEhIi6fdf2g888IBWrlyp5ORktW7dWo0bN1Z2drb+9a9/qUWLFsUeQFD2NG/eXMnJyapVq5bq1q1rM3l4eKhOnToqX768zXvl7Nmz+v7776/bZv6dzPNv5CKpwA0bAVjDx8fH5rOXlZWlY8eOFblOvXr1CnwnuHrez89PAQEB+uGHHwr8nggKCrrhvjk7Oys3N/eG6wOlKTg4WCdOnNCJEyfMZQcPHlRGRob53ae479GSjne1a9dW+fLlbT5nmZmZNmNrs2bNlJubq9OnTxf43BXnCUV87m4OoRvF8swzzygvL0/PP/+8Dh06pHXr1umtt96S9L+/ROPOcffdd6tz58567rnntG3bNu3du1fPPvus7rrrLnXu3NmsFx4erk8++URNmzaVp6enHB0d1aZNGy1atEht27a14x6gtERHR+vXX39Vt27dtHv3bh09elTr1q1Tnz59lJubK09PT/Xr108jRozQpk2bdODAAfXu3dv8635h3Nzc1KpVK/OGMfHx8RozZswt3CvgztWuXTv9+9//1ldffaX9+/erV69eBY6SXWvw4MH64IMPtHDhQh0+fFiTJ0/Wvn37bL4fTJgwQTExMXr77bf1/fffa//+/VqwYIGmT59+w32rVauWzp07p40bN+rnn3/myQS4pSIiItSoUSN1795d3377rXbt2qWePXuqbdu25qnftWrV0rFjx5SUlKSff/7Z5mBVYUo63lWoUEG9evXSiBEjtHnzZiUnJ6tfv35ydHQ0P3f33HOPunfvrp49e+qzzz7TsWPHtGvXLsXExGj16tU3vN+1atXSvn37lJKSop9//vm6R+JROEI3isXLy0tffPGFkpKS1LRpU7366qsaO3asJNlc5407x4IFCxQaGqpHH31UYWFhMgxDa9assTlNqW3btsrNzbW5djs8PLzAMty+AgICtH37duXm5qpDhw5q1KiRhgwZoooVK5rB+s0339QDDzygTp06KSIiQq1bt1ZoaGiR7X744Ye6cuWKQkNDNWTIEE2ePPlW7A5wxxs9erTatm2rRx99VFFRUerSpYvN9Z6F6d69u0aPHq1XXnlFzZs317Fjx9S7d2+b7wf9+/fX/PnztWDBAjVq1Eht27ZVbGxssY50/+Uvf9GAAQP01FNPycfHR1OnTi3xfgLF5eDgoJUrV6pSpUpq06aNIiIiVLt2bS1dutSs07VrVz388MN68MEH5ePjo08++eQP2y3peDd9+nSFhYXp0UcfVUREhO6//37zkXz5FixYoJ49e2r48OGqV6+eunTpot27d6tmzZo3vN/PPfec6tWrpxYtWsjHx0fbt2+/4XUhORicrI+btGjRIvOZmW5ubvbuDgAAKCMeeugh+fv769///re9uwLcEc6fP6+77rpL06ZNU79+/ezdHfx/3EgNxfbRRx+pdu3auuuuu7R3716NGjVKTz75JIEbAIA72IULFzRv3jxFRkbKyclJn3zyiTZs2KC4uDh7dw3409qzZ4++++473XfffcrMzNTEiRMlyeYyP9gfoRvFlpaWprFjxyotLU3VqlXTE088oSlTpti7WwAAwI4cHBy0Zs0aTZkyRZcuXVK9evX03//+VxEREfbuGmBXixYt0gsvvFBoWWBgoJKTk2+q/bfeekspKSlydnZWaGiovvrqK1WtWvWm2kTp4vRyAAAAALDIb7/9pvT09ELLypcvr8DAwFvcI9xqhG4AAAAAACzC3csBAAAAALAIoRsAAAAAAIsQugEAAAAAsAihGwAAAAAAixC6ARTb8ePH5eDgoKSkpJtuy8HBQStWrLjpdgAAAICyiLuXAyi23NxcnTlzRlWrVlW5cuVuqq20tDRVqlRJLi4uOn78uIKCgrRnzx41bdq0dDoLAAAA2BFHugEUS05OjpycnOTv739TgTsnJ0eS5O/vLxcXl9LqHgAAd6zLly/buwsACkHoBu5w4eHhGjRokAYNGiRvb29VrVpVr732mvJPgqlVq5YmTZqknj17ysvLS88//3yhp5fHx8frvvvuk4uLi6pVq6a///3vunLlSoHtDBkyRFWrVlVkZKQk29PLg4KCJEnNmjWTg4ODwsPDtXXrVpUvX15paWk2/R4yZIgeeOABC18ZAADsq1atWpo5c6bNsqZNm2r8+PGSfh9D586dq7/+9a/y8PDQlClTJEmTJ0+Wr6+vKlSooP79++vvf/97gTPI5s+fr+DgYLm6uqp+/fqaM2eOWZY/zn/22Wd68MEH5e7uriZNmighIcGmje3btys8PFzu7u6qVKmSIiMjdfbsWX300UeqUqWKsrOzbep36dJFPXr0KJ0XB7iNELoBaOHChSpXrpx27dqlWbNmafr06Zo/f75Z/tZbb6lJkybas2ePXnvttQLr//TTT3rkkUd07733au/evZo7d64++OADTZ48ucB2nJ2dtX37ds2bN69AO7t27ZIkbdiwQadOndJnn32mNm3aqHbt2vr3v/9t1rt8+bIWLVqkvn37ltZLAADAbWn8+PF67LHHtH//fvXt21eLFi3SlClT9MYbbygxMVE1a9bU3LlzbdZZtGiRxo4dqylTpujQoUP65z//qddee00LFy60qffqq6/qlVdeUVJSku655x5169bN/IN6UlKS2rdvr5CQECUkJGjbtm3q1KmTcnNz9cQTTyg3N1eff/652dbp06e1evVqxm7cmQwAd7S2bdsawcHBRl5enrls1KhRRnBwsGEYhhEYGGh06dLFZp1jx44Zkow9e/YYhmEY//jHP4x69erZtDF79mzD09PTyM3NNbfTrFmzAtuXZCxfvrzQdvO98cYbZn8MwzD++9//Gp6ensa5c+dKvN8AAJR1gYGBxowZM2yWNWnSxBg3bpxhGL+PoUOGDLEpb9mypREdHW2z7P777zeaNGliztepU8dYvHixTZ1JkyYZYWFhhmH8bzyeP3++WZ6cnGxIMg4dOmQYhmF069bNuP/++6/b94EDBxodO3Y056dNm2bUrl3b5rsCcKfgSDcAtWrVSg4ODuZ8WFiYDh8+rNzcXElSixYtilz/0KFDCgsLs2nj/vvv17lz5/Tjjz+ay0JDQ0vUv969e+vIkSPauXOnJCk2NlZPPvmkPDw8StQeAAB/FteO0SkpKbrvvvtsll09f/78eR09elT9+vWTp6enOU2ePFlHjx61Wa9x48bm/6tVqybp9yPW0v+OdF/Pc889p/Xr1+unn36S9PvY3bt3b5vvCsCd4uZuOwzgjlBa4bak7fj6+qpTp05asGCBgoKC9OWXX2rLli2l0icAAMoqR0dH8x4r+a69WVpxx9Zz585Jkt5//321bNnSpszJyclmvnz58ub/88NyXl6eJMnNza3I7TRr1kxNmjTRRx99pA4dOig5OVmrV68uVl+BPwuOdAPQ119/bTO/c+dO3X333QUG3+sJDg5WQkKCzReD7du3q0KFCqpevfoN98PZ2VmSzCPsV+vfv7+WLl2q9957T3Xq1NH9999/w+0CAHA78vHx0alTp8z5rKwsHTt2rMh16tWrp927d9ssu3rez89PAQEB+uGHH1S3bl2bKf+GpjeicePG2rhxY5F1+vfvr9jYWC1YsEARERGqUaPGDbcP/JkQugEoNTVVw4YNU0pKij755BO98847evnll294/RdffFEnTpzQ4MGD9d1332nlypUaN26chg0bJkfHG/814+vrKzc3N61du1bp6enKzMw0yyIjI+Xl5aXJkyerT58+xdo/AABuR+3atdO///1vffXVV9q/f7969er1h38QHzx4sD744AMtXLhQhw8f1uTJk7Vv3z6b07onTJigmJgYvf322/r++++1f/9+LViwQNOnT7/hvo0ePVq7d+/Wiy++qH379um7777T3Llz9fPPP5t1nnnmGf344496//33uYEa7miEbgDq2bOnLl68qPvuu0/R0dF6+eWX9fzzz9/w+nfddZfWrFmjXbt2qUmTJhowYID69eunMWPGFKsf5cqV09tvv61//etfCggIUOfOnc0yR0dH9e7dW7m5uerZs2ex2gUA4HY0evRotW3bVo8++qiioqLUpUsX1alTp8h1unfvrtGjR+uVV15R8+bNdezYMfXu3Vuurq5mnf79+2v+/PlasGCBGjVqpLZt2yo2NrZYR7rvuecerV+/Xnv37tV9992nsLAwrVy5UuXK/e/qVW9vb3Xt2lWenp7q0qVLsfcf+LNwMK69UATAHSU8PFxNmzYt8BzQsqhfv346c+aMzSNIAABA0R566CH5+/vbPH7zVmnfvr0aNGigt99++5ZvGygruJEagDIvMzNT+/fv1+LFiwncAAAU4cKFC5o3b54iIyPl5OSkTz75RBs2bFBcXNwt7cfZs2e1ZcsWbdmyRXPmzLml2wbKGkI3gDKvc+fO2rVrlwYMGKCHHnrI3t0BAKDMcnBw0Jo1azRlyhRdunRJ9erV03//+19FRETc0n40a9ZMZ8+e1RtvvKF69erd0m0DZQ2nlwMAAAAAYBFupAYAAAAAgEUI3QAAAAAAWITQDQAAAACARQjdAAAAAABYhNANAAAAAIBFCN0AAAAAAFiE0A0AAAAAgEUI3QAAAAAAWITQDQAAAACARf4fETbzUncfvyYAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 1000x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize data\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Check the distribution of the labels\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 6))\n",
    "sns.countplot(x='priority', data=df_combined, ax=axs[0])\n",
    "axs[0].set_title('Priority Distribution')\n",
    "sns.countplot(x='urgency', data=df_combined, ax=axs[1])\n",
    "axs[1].set_title('Urgency Distribution')\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Priority as feature "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import numpy as np\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Function to get BERT embeddings for each sentence\n",
    "def get_bert_embeddings(sentences):\n",
    "    embeddings = []\n",
    "    for sentence in sentences:\n",
    "        # Tokenize the sentence\n",
    "        inputs = tokenizer(sentence, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "        \n",
    "        # Get the BERT embeddings (output of the last hidden layer)\n",
    "        with torch.no_grad():\n",
    "            outputs = bert_model(**inputs)\n",
    "            last_hidden_state = outputs.last_hidden_state  # shape: [batch_size, seq_length, hidden_size]\n",
    "        \n",
    "        # Use the [CLS] token's embedding as the sentence representation\n",
    "        cls_embedding = last_hidden_state[:, 0, :].numpy()\n",
    "        embeddings.append(cls_embedding)\n",
    "    \n",
    "    return np.vstack(embeddings)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m data \u001b[38;5;241m=\u001b[39m df_combined[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprocessed_text\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Get BERT embeddings\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m X_bert \u001b[38;5;241m=\u001b[39m \u001b[43mget_bert_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[23], line 18\u001b[0m, in \u001b[0;36mget_bert_embeddings\u001b[1;34m(sentences)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Get the BERT embeddings (output of the last hidden layer)\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 18\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mbert_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m     last_hidden_state \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state  \u001b[38;5;66;03m# shape: [batch_size, seq_length, hidden_size]\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Use the [CLS] token's embedding as the sentence representation\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kripa\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kripa\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kripa\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1142\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1135\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[0;32m   1136\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[0;32m   1137\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[0;32m   1138\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[0;32m   1139\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[0;32m   1140\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m-> 1142\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1143\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1145\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1149\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1150\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1151\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1152\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1153\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1154\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1155\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kripa\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kripa\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kripa\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:695\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    684\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    685\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    686\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    692\u001b[0m         output_attentions,\n\u001b[0;32m    693\u001b[0m     )\n\u001b[0;32m    694\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 695\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    696\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    697\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    698\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    699\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    700\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    701\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    703\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    705\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    706\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32mc:\\Users\\kripa\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kripa\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kripa\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:585\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    574\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    575\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    582\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m    583\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[0;32m    584\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 585\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    587\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    588\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    589\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    590\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    591\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    592\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    594\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kripa\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kripa\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kripa\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:524\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    505\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    506\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    507\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    513\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    514\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m    515\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself(\n\u001b[0;32m    516\u001b[0m         hidden_states,\n\u001b[0;32m    517\u001b[0m         attention_mask,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    522\u001b[0m         output_attentions,\n\u001b[0;32m    523\u001b[0m     )\n\u001b[1;32m--> 524\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mself_outputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    525\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n\u001b[0;32m    526\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[1;32mc:\\Users\\kripa\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kripa\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kripa\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:466\u001b[0m, in \u001b[0;36mBertSelfOutput.forward\u001b[1;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[0;32m    465\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor, input_tensor: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m--> 466\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    467\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(hidden_states)\n\u001b[0;32m    468\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLayerNorm(hidden_states \u001b[38;5;241m+\u001b[39m input_tensor)\n",
      "File \u001b[1;32mc:\\Users\\kripa\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kripa\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kripa\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data = df_combined['processed_text'].tolist()\n",
    "\n",
    "# Get BERT embeddings\n",
    "X_bert = get_bert_embeddings(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = df['processed_text']\n",
    "y = df['urgency']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import numpy as np\n",
    " \n",
    "# Load BERT tokenizer and model\n",
    "model_name = \"bert-base-uncased\"  # You can replace this with other models like 'roberta-base' or 'distilbert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    " \n",
    "# Function to tokenize and generate BERT embeddings\n",
    "def bert_vectorize(texts):\n",
    "    \"\"\"\n",
    "    Converts a list of texts into BERT embeddings.\n",
    "    :param texts: List of text strings\n",
    "    :return: Numpy array of embeddings\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    # Use the mean of the token embeddings (pooled output) for the final sentence representation\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "    return embeddings.numpy()\n",
    " \n",
    "# Convert your dataset to BERT embeddings\n",
    "X_train_bert = bert_vectorize(X_train.values.tolist())\n",
    "X_test_bert = bert_vectorize(X_test.values.tolist())\n",
    " \n",
    "# Check the shapes of the resulting arrays\n",
    "print(\"X_train_bert shape:\", X_train_bert.shape)\n",
    "print(\"X_test_bert shape:\", X_test_bert.shape)\n",
    " \n",
    " \n",
    "# Load BERT tokenizer and model\n",
    "model_name = \"bert-base-uncased\"  # You can replace this with other models like 'roberta-base' or 'distilbert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    " \n",
    "# Function to tokenize and generate BERT embeddings\n",
    "def bert_vectorize(texts):\n",
    "    \"\"\"\n",
    "    Converts a list of texts into BERT embeddings.\n",
    "    :param texts: List of text strings\n",
    "    :return: Numpy array of embeddings\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    # Use the mean of the token embeddings (pooled output) for the final sentence representation\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "    return embeddings.numpy()\n",
    " \n",
    "# Convert your dataset to BERT embeddings\n",
    "X_train_bert = bert_vectorize(X_train.values.tolist())\n",
    "X_test_bert = bert_vectorize(X_test.values.tolist())\n",
    " \n",
    "# Check the shapes of the resulting arrays\n",
    "print(\"X_train_bert shape:\", X_train_bert.shape)\n",
    "print(\"X_test_bert shape:\", X_test_bert.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Example of adding TF-IDF features (you can skip this if you already have them)\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X_tfidf = vectorizer.fit_transform(data).toarray()\n",
    "\n",
    "# Combine BERT embeddings with TF-IDF features\n",
    "X_combined = np.hstack([X_tfidf, X_bert])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  not_urgent       0.96      0.93      0.94       156\n",
      "      urgent       0.93      0.96      0.95       157\n",
      "\n",
      "    accuracy                           0.95       313\n",
      "   macro avg       0.95      0.95      0.95       313\n",
      "weighted avg       0.95      0.95      0.95       313\n",
      "\n",
      "SVM Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  not_urgent       1.00      0.97      0.98       156\n",
      "      urgent       0.97      1.00      0.98       157\n",
      "\n",
      "    accuracy                           0.98       313\n",
      "   macro avg       0.98      0.98      0.98       313\n",
      "weighted avg       0.98      0.98      0.98       313\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y = df['urgency'].values\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_combined, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Logistic Regression model\n",
    "logreg_model = LogisticRegression()\n",
    "logreg_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_logreg = logreg_model.predict(X_test)\n",
    "print(\"Logistic Regression Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_logreg))\n",
    "\n",
    "# Train SVM model\n",
    "svm_model = SVC(kernel='linear')\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_svm = svm_model.predict(X_test)\n",
    "print(\"SVM Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_svm))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example prediction\n",
    "# New example text to test the model\n",
    "example_text = [\n",
    "    \"we need to find the dataset ourselves based on the use case\",\n",
    "    \"Are we instructed anything on POC?\",\n",
    "    \"I am not able to access the data in the shared folder\",\n",
    "    \"nicee emergency and non emergency\",\n",
    "    \"rasied pr for bug fixes. let me know when it is approved. no hurry\",\n",
    "    \"once everything works fine we will push to stage\"\n",
    "]\n",
    "\n",
    "# Get BERT embeddings\n",
    "X_bert_example = get_bert_embeddings(example_text)\n",
    "\n",
    "# Get TF-IDF features\n",
    "X_tfidf_example = vectorizer.transform(example_text).toarray()\n",
    "\n",
    "# Combine BERT embeddings with TF-IDF features\n",
    "X_combined_example = np.hstack([X_tfidf_example, X_bert_example])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Prediction : ['not_urgent' 'not_urgent' 'not_urgent' 'not_urgent' 'urgent' 'not_urgent']\n"
     ]
    }
   ],
   "source": [
    "# make prediction\n",
    "# Predict with Logistic Regression model\n",
    "y_pred_logreg_example = logreg_model.predict(X_combined_example)\n",
    "print(\"Logistic Regression Prediction :\", y_pred_logreg_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Prediction (0: Non-Urgent, 1: Urgent): ['not_urgent' 'not_urgent' 'urgent' 'not_urgent' 'not_urgent' 'not_urgent']\n"
     ]
    }
   ],
   "source": [
    "y_pred_svm_example = svm_model.predict(X_combined_example)\n",
    "print(\"SVM Prediction (0: Non-Urgent, 1: Urgent):\", y_pred_svm_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Using BERT\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    " \n",
    " \n",
    "class BertTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, model='bert-base-nli-mean-tokens'):\n",
    "        self.model_name = model\n",
    "        self.model = SentenceTransformer(self.model_name)\n",
    " \n",
    "    def fit(self, X, y=None):\n",
    "        # No training is required for SentenceTransformer\n",
    "        return self\n",
    " \n",
    "    def transform(self, X):\n",
    "        # Ensure X is converted to a list of strings\n",
    "        if isinstance(X, np.ndarray):\n",
    "            X = X.tolist()\n",
    "        elif isinstance(X, pd.Series):  # If input is a pandas series\n",
    "            X = X.to_list()\n",
    " \n",
    "        # Generate sentence embeddings\n",
    "        embeddings = self.model.encode(X, show_progress_bar=False)\n",
    "        return np.array(embeddings)  # Ensure the output is a numpy array\n",
    "# Define pipeline with BERT vectorizer and XGBoost\n",
    "pipeline = Pipeline([\n",
    "    ('bert_vectorizer', BertTransformer()),\n",
    "    ('classifier', RandomForestClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# Fit the pipeline\n",
    "pipeline.fit(X_train_urgency, y_train_urgency)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = pipeline.predict(X_test_urgency)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Classification Report:\\n\", classification_report(y_test_urgency, y_pred))\n",
    "transformer = BertTransformer()\n",
    "\n",
    "transformer.fit(X_train_urgency['text'])\n",
    "\n",
    "X_train_transformed = transformer.transform(X_train_urgency['text'])\n",
    "X_test_transformed = transformer.transform(X_test_urgency['text'])\n",
    "\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "model.fit(X_train_transformed, y_train_urgency)\n",
    "\n",
    "y_pred = model.predict(X_test_transformed)\n",
    "\n",
    "print(\"Classification Report:\\n\", classification_report(y_test_urgency, y_pred))\n",
    "# example \n",
    "example_text = pd.DataFrame(\n",
    "   { 'texts': [\n",
    "   'This is an urgent issue',\n",
    "    'I need help with my account right now without delay',\n",
    "    'I have a question about my account',\n",
    "    'I cannot access my account. Please help me',\n",
    "    'I am unable to access my account. I have tried resetting my password but it is not working. I need help urgently',\n",
    "    \"Query regarding my account. Please help me\",\n",
    "    \"The server is down. Please fix it as soon as possible\",\n",
    "    \"System Issue. Need help before the end of the day\",\n",
    "    \"Jira tickets need to be updated. Please do it as soon as possible. no hurry\",\n",
    "    \"Not Critical. Please ensure system is up and running without any issues before the end of the day.\",\n",
    "    ],\n",
    "    'labels': [1, 1, 0, 1, 1, 0, 1, 1, 0, 0]}\n",
    ")\n",
    "\n",
    "example_text_transformed = transformer.transform(example_text['texts'])\n",
    "\n",
    "print(f\"Prediction: \\n{model.predict(example_text_transformed)}\")\n",
    "print(f\"Classification Report: \\n{classification_report(example_text['labels'], model.predict(example_text_transformed))}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
