{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1564, 5)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('csv/processed_tickets_en.csv')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kripa/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f7559830b704772835b22fee45857e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90b27a6f23aa401e9d9e7f696b5f8cbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dfca0645fb54787a9e8e8bbaa45aadb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "171b3295365e4f6fb6f9a8099b92f584",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bd916dc13ad41139228bd67c0582824",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import numpy as np\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Function to get BERT embeddings for each sentence\n",
    "def get_bert_embeddings(sentences):\n",
    "    embeddings = []\n",
    "    for sentence in sentences:\n",
    "        # Tokenize the sentence\n",
    "        inputs = tokenizer(sentence, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "        \n",
    "        # Get the BERT embeddings (output of the last hidden layer)\n",
    "        with torch.no_grad():\n",
    "            outputs = bert_model(**inputs)\n",
    "            last_hidden_state = outputs.last_hidden_state  # shape: [batch_size, seq_length, hidden_size]\n",
    "        \n",
    "        # Use the [CLS] token's embedding as the sentence representation\n",
    "        cls_embedding = last_hidden_state[:, 0, :].numpy()\n",
    "        embeddings.append(cls_embedding)\n",
    "    \n",
    "    return np.vstack(embeddings)\n",
    "\n",
    "# Example usage: Assume `data` is your list of ticket descriptions\n",
    "data = df['processed_text'].tolist()\n",
    "\n",
    "# Get BERT embeddings\n",
    "X_bert = get_bert_embeddings(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = df['processed_text']\n",
    "y = df['urgency']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import numpy as np\n",
    " \n",
    "# Load BERT tokenizer and model\n",
    "model_name = \"bert-base-uncased\"  # You can replace this with other models like 'roberta-base' or 'distilbert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    " \n",
    "# Function to tokenize and generate BERT embeddings\n",
    "def bert_vectorize(texts):\n",
    "    \"\"\"\n",
    "    Converts a list of texts into BERT embeddings.\n",
    "    :param texts: List of text strings\n",
    "    :return: Numpy array of embeddings\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    # Use the mean of the token embeddings (pooled output) for the final sentence representation\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "    return embeddings.numpy()\n",
    " \n",
    "# Convert your dataset to BERT embeddings\n",
    "X_train_bert = bert_vectorize(X_train.values.tolist())\n",
    "X_test_bert = bert_vectorize(X_test.values.tolist())\n",
    " \n",
    "# Check the shapes of the resulting arrays\n",
    "print(\"X_train_bert shape:\", X_train_bert.shape)\n",
    "print(\"X_test_bert shape:\", X_test_bert.shape)\n",
    " \n",
    " \n",
    "# Load BERT tokenizer and model\n",
    "model_name = \"bert-base-uncased\"  # You can replace this with other models like 'roberta-base' or 'distilbert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    " \n",
    "# Function to tokenize and generate BERT embeddings\n",
    "def bert_vectorize(texts):\n",
    "    \"\"\"\n",
    "    Converts a list of texts into BERT embeddings.\n",
    "    :param texts: List of text strings\n",
    "    :return: Numpy array of embeddings\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    # Use the mean of the token embeddings (pooled output) for the final sentence representation\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "    return embeddings.numpy()\n",
    " \n",
    "# Convert your dataset to BERT embeddings\n",
    "X_train_bert = bert_vectorize(X_train.values.tolist())\n",
    "X_test_bert = bert_vectorize(X_test.values.tolist())\n",
    " \n",
    "# Check the shapes of the resulting arrays\n",
    "print(\"X_train_bert shape:\", X_train_bert.shape)\n",
    "print(\"X_test_bert shape:\", X_test_bert.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Example of adding TF-IDF features (you can skip this if you already have them)\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X_tfidf = vectorizer.fit_transform(data).toarray()\n",
    "\n",
    "# Combine BERT embeddings with TF-IDF features\n",
    "X_combined = np.hstack([X_tfidf, X_bert])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  not_urgent       0.96      0.93      0.94       156\n",
      "      urgent       0.93      0.96      0.95       157\n",
      "\n",
      "    accuracy                           0.95       313\n",
      "   macro avg       0.95      0.95      0.95       313\n",
      "weighted avg       0.95      0.95      0.95       313\n",
      "\n",
      "SVM Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  not_urgent       1.00      0.97      0.98       156\n",
      "      urgent       0.97      1.00      0.98       157\n",
      "\n",
      "    accuracy                           0.98       313\n",
      "   macro avg       0.98      0.98      0.98       313\n",
      "weighted avg       0.98      0.98      0.98       313\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y = df['urgency'].values\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_combined, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Logistic Regression model\n",
    "logreg_model = LogisticRegression()\n",
    "logreg_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_logreg = logreg_model.predict(X_test)\n",
    "print(\"Logistic Regression Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_logreg))\n",
    "\n",
    "# Train SVM model\n",
    "svm_model = SVC(kernel='linear')\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_svm = svm_model.predict(X_test)\n",
    "print(\"SVM Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_svm))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example prediction\n",
    "# New example text to test the model\n",
    "example_text = [\n",
    "    \"we need to find the dataset ourselves based on the use case\",\n",
    "    \"Are we instructed anything on POC?\",\n",
    "    \"I am not able to access the data in the shared folder\",\n",
    "    \"nicee emergency and non emergency\",\n",
    "    \"rasied pr for bug fixes. let me know when it is approved. no hurry\",\n",
    "    \"once everything works fine we will push to stage\"\n",
    "]\n",
    "\n",
    "# Get BERT embeddings\n",
    "X_bert_example = get_bert_embeddings(example_text)\n",
    "\n",
    "# Get TF-IDF features\n",
    "X_tfidf_example = vectorizer.transform(example_text).toarray()\n",
    "\n",
    "# Combine BERT embeddings with TF-IDF features\n",
    "X_combined_example = np.hstack([X_tfidf_example, X_bert_example])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Prediction : ['not_urgent' 'not_urgent' 'not_urgent' 'not_urgent' 'urgent' 'not_urgent']\n"
     ]
    }
   ],
   "source": [
    "# make prediction\n",
    "# Predict with Logistic Regression model\n",
    "y_pred_logreg_example = logreg_model.predict(X_combined_example)\n",
    "print(\"Logistic Regression Prediction :\", y_pred_logreg_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Prediction (0: Non-Urgent, 1: Urgent): ['not_urgent' 'not_urgent' 'urgent' 'not_urgent' 'not_urgent' 'not_urgent']\n"
     ]
    }
   ],
   "source": [
    "y_pred_svm_example = svm_model.predict(X_combined_example)\n",
    "print(\"SVM Prediction (0: Non-Urgent, 1: Urgent):\", y_pred_svm_example)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
